{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3945b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PKSampler(Sampler):\n",
    "    \"\"\"\n",
    "    PK Sampler for Person Re-ID: P persons × K images per person\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, P=16, K=4):\n",
    "        self.data_source = data_source\n",
    "        self.P = P  # Number of persons per batch\n",
    "        self.K = K  # Number of images per person\n",
    "        \n",
    "        # Group samples by person ID\n",
    "        self.pid_to_indices = defaultdict(list)\n",
    "        for idx, (_, pid) in enumerate(data_source.samples):\n",
    "            self.pid_to_indices[pid].append(idx)\n",
    "        \n",
    "        # Filter out persons with less than K images\n",
    "        self.valid_pids = [pid for pid, indices in self.pid_to_indices.items() \n",
    "                          if len(indices) >= self.K]\n",
    "        \n",
    "        if len(self.valid_pids) < self.P:\n",
    "            raise ValueError(f\"Not enough persons with at least {self.K} images. \"\n",
    "                           f\"Found {len(self.valid_pids)}, need {self.P}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Calculate number of batches\n",
    "        num_batches = len(self.valid_pids) // self.P\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            # Randomly select P persons\n",
    "            selected_pids = random.sample(self.valid_pids, self.P)\n",
    "            \n",
    "            batch_indices = []\n",
    "            for pid in selected_pids:\n",
    "                # Randomly select K images for this person\n",
    "                available_indices = self.pid_to_indices[pid]\n",
    "                selected_indices = random.sample(available_indices, \n",
    "                                               min(self.K, len(available_indices)))\n",
    "                batch_indices.extend(selected_indices)\n",
    "            \n",
    "            # Shuffle within batch to avoid ordering bias\n",
    "            random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_pids) // self.P\n",
    "\n",
    "class PersonReIDTrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training set: expects structure Dataset/train/<pid>/*.jpg\n",
    "    Returns (image, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label)\n",
    "        self.label_map = {}  # pid (str) -> label (int)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        pids = sorted(os.listdir(self.root_dir))\n",
    "        self.label_map = {pid: idx for idx, pid in enumerate(pids)}\n",
    "        for pid in pids:\n",
    "            pid_dir = os.path.join(self.root_dir, pid)\n",
    "            if not os.path.isdir(pid_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(pid_dir):\n",
    "                if fname.lower().endswith('.jpg'):\n",
    "                    self.samples.append((os.path.join(pid_dir, fname), self.label_map[pid]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "class PersonReIDTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for query/gallery set: expects structure Dataset/query/*.jpg or Dataset/gallery/*.jpg\n",
    "    Returns (image, label, cam_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label, cam_id)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        for fname in os.listdir(self.dir_path):\n",
    "            if fname.lower().endswith('.jpg'):\n",
    "                # Example: 0001_c1s1_001051_00.jpg\n",
    "                parts = fname.split('_')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                label = int(parts[0])\n",
    "                cam_id = int(parts[1][1])  # e.g., c1 -> 1\n",
    "                self.samples.append((os.path.join(self.dir_path, fname), label, cam_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, cam_id = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label, cam_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ff540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =========================\n",
    "# 1. Spatial Semantic Clustering (CNN Adaptation)\n",
    "# =========================\n",
    "class SpatialSemanticClustering(nn.Module):\n",
    "    \"\"\"-level semantic clustering.\n",
    "    Works with feature maps instead of transformer tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, num_semantic_parts=3, momentum=0.99):\n",
    "        super(SpatialSemanticClustering, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_semantic_parts = num_semantic_parts  # upper_body, lower_body, shoes\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Semantic head for classification (adapted from SOLIDER)\n",
    "        self.semantic_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 2),\n",
    "            nn.BatchNorm1d(feature_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 2, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, num_semantic_parts + 1)  # +1 for background\n",
    "        )\n",
    "        \n",
    "        # Moving averages for foreground/background clustering\n",
    "        self.register_buffer('fg_threshold', torch.tensor(0.5))\n",
    "        \n",
    "    def spatial_semantic_labeling(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Adapt SOLIDER's human prior knowledge to CNN feature maps.\n",
    "        Uses spatial coordinates (y-axis) to assign semantic labels.\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        # Create coordinate grids for semantic assignment\n",
    "        y_coords = torch.linspace(0, 1, H, device=device).view(H, 1).expand(H, W)\n",
    "        x_coords = torch.linspace(0, 1, W, device=device).view(1, W).expand(H, W)\n",
    "        \n",
    "        # Human prior: spatial semantic assignment based on y-coordinates\n",
    "        semantic_labels = torch.zeros(H, W, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Upper body (head, chest, arms): top 40%\n",
    "        upper_mask = y_coords < 0.4\n",
    "        semantic_labels[upper_mask] = 0\n",
    "        \n",
    "        # Lower body (waist, thighs): middle 40%\n",
    "        middle_mask = (y_coords >= 0.4) & (y_coords < 0.8)\n",
    "        semantic_labels[middle_mask] = 1\n",
    "        \n",
    "        # Shoes (calves, feet): bottom 20%\n",
    "        lower_mask = y_coords >= 0.8\n",
    "        semantic_labels[lower_mask] = 2\n",
    "        \n",
    "        return semantic_labels\n",
    "    \n",
    "    def foreground_background_clustering(self, feature_maps):\n",
    "        \"\"\"\n",
    "        Separate foreground and background based on feature magnitude.\n",
    "        Adapted from SOLIDER's background filtering approach.\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        \n",
    "        # Calculate feature magnitude (L2 norm across channels)\n",
    "        feature_magnitude = torch.norm(feature_maps, dim=1)  # [B, H, W]\n",
    "        \n",
    "        # Adaptive threshold based on mean + std\n",
    "        batch_mean = feature_magnitude.mean(dim=(1, 2), keepdim=True)\n",
    "        batch_std = feature_magnitude.std(dim=(1, 2), keepdim=True)\n",
    "        fg_threshold = batch_mean + 0.5 * batch_std\n",
    "        \n",
    "        # Create foreground mask\n",
    "        fg_mask = feature_magnitude > fg_threshold  # [B, H, W]\n",
    "        \n",
    "        return fg_mask\n",
    "    \n",
    "    def generate_pseudo_labels(self, feature_maps, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Generate pseudo semantic labels for CNN feature maps.\n",
    "        Combines spatial priors with foreground/background separation.\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        # Use teacher features for clustering if available (student-teacher setup)\n",
    "        clustering_features = teacher_features if teacher_features is not None else feature_maps\n",
    "        \n",
    "        # Step 1: Foreground/Background separation\n",
    "        fg_mask = self.foreground_background_clustering(clustering_features)\n",
    "        \n",
    "        # Step 2: Spatial semantic labeling based on human priors\n",
    "        spatial_labels = self.spatial_semantic_labeling(clustering_features)\n",
    "        \n",
    "        # Step 3: Combine foreground mask with spatial labels\n",
    "        pseudo_labels = torch.full((B, H, W), self.num_semantic_parts, \n",
    "                                 dtype=torch.long, device=device)  # background class\n",
    "        \n",
    "        for b in range(B):\n",
    "            fg_positions = fg_mask[b]\n",
    "            pseudo_labels[b][fg_positions] = spatial_labels[fg_positions]\n",
    "        \n",
    "        return pseudo_labels, fg_mask\n",
    "    \n",
    "    def forward(self, student_features, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass for semantic clustering and classification.\n",
    "        \"\"\"\n",
    "        B, C, H, W = student_features.shape\n",
    "        \n",
    "        # Generate pseudo semantic labels\n",
    "        pseudo_labels, fg_mask = self.generate_pseudo_labels(student_features, teacher_features)\n",
    "        \n",
    "        # Flatten features for semantic classification\n",
    "        student_flat = student_features.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, C]\n",
    "        labels_flat = pseudo_labels.reshape(-1)  # [B*H*W]\n",
    "        \n",
    "        # Semantic classification\n",
    "        semantic_logits = self.semantic_head(student_flat)  # [B*H*W, num_classes]\n",
    "        \n",
    "        # Calculate semantic loss (cross-entropy)\n",
    "        semantic_loss = F.cross_entropy(semantic_logits, labels_flat, reduction='mean')\n",
    "        \n",
    "        return {\n",
    "            'semantic_loss': semantic_loss,\n",
    "            'pseudo_labels': pseudo_labels,\n",
    "            'foreground_mask': fg_mask,\n",
    "            'semantic_logits': semantic_logits.view(B, H, W, -1)\n",
    "        }\n",
    "\n",
    "# =========================\n",
    "# 2. Semantic Controller (CNN Adaptation)\n",
    "# =========================\n",
    "class SemanticController(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN adaptation of SOLIDER's semantic controller.\n",
    "    Controls the ratio of semantic vs appearance information in feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(SemanticController, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Lambda encoding networks (adapted from SOLIDER)\n",
    "        self.weight_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),\n",
    "            nn.Softplus()  # Ensures positive weights\n",
    "        )\n",
    "        \n",
    "        self.bias_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, feature_maps, lambda_val):\n",
    "        \"\"\"\n",
    "        Apply semantic control to feature maps.\n",
    "        lambda_val: 0 = appearance-focused, 1 = semantic-focused\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        # Handle different lambda input formats\n",
    "        if isinstance(lambda_val, (int, float)):\n",
    "            lambda_tensor = torch.tensor([lambda_val], device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            lambda_tensor = lambda_val.to(device)\n",
    "        \n",
    "        if lambda_tensor.dim() == 0:\n",
    "            lambda_tensor = lambda_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Encode lambda into weights and biases\n",
    "        weights = self.weight_encoder(lambda_tensor.unsqueeze(-1))  # [1, C] or [B, C]\n",
    "        biases = self.bias_encoder(lambda_tensor.unsqueeze(-1))     # [1, C] or [B, C]\n",
    "        \n",
    "        # Apply channel-wise modulation (broadcasting across spatial dimensions)\n",
    "        if weights.size(0) == 1:\n",
    "            weights = weights.expand(B, -1)\n",
    "            biases = biases.expand(B, -1)\n",
    "        \n",
    "        # Reshape for broadcasting: [B, C, 1, 1]\n",
    "        weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "        biases = biases.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Apply semantic control: controlled_features = weights * features + biases\n",
    "        controlled_features = weights * feature_maps + biases\n",
    "        \n",
    "        return controlled_features\n",
    "\n",
    "# =========================\n",
    "# 3. SOLIDER CNN Block (ResNet Block with Semantic Control)\n",
    "# =========================\n",
    "class SOLIDERCNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet block integrated with SOLIDER semantic control.\n",
    "    Maintains ResNet structure while adding semantic controllability.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(SOLIDERCNNBlock, self).__init__()\n",
    "        \n",
    "        # Standard ResNet block components\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # SOLIDER semantic controller\n",
    "        self.semantic_controller = SemanticController(out_channels)\n",
    "        \n",
    "    def forward(self, x, lambda_val=0.5):\n",
    "        identity = x\n",
    "        \n",
    "        # Standard ResNet forward pass\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply downsampling if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Apply semantic control before adding residual\n",
    "        out = self.semantic_controller(out, lambda_val)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# =========================\n",
    "# 4. Multi-Scale Feature Fusion\n",
    "# =========================\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses features from different ResNet stages for richer semantic understanding.\n",
    "    Inspired by SOLIDER's multi-scale approach adapted for CNNs.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dims=[256, 512, 1024, 2048], output_dim=2048):\n",
    "        super(MultiScaleFeatureFusion, self).__init__()\n",
    "        self.feature_dims = feature_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Projection layers to align feature dimensions\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(dim, output_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(output_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for dim in feature_dims\n",
    "        ])\n",
    "        \n",
    "        # Attention weights for different scales\n",
    "        self.scale_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(output_dim, output_dim // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_dim // 4, len(feature_dims), 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, multi_scale_features):\n",
    "        \"\"\"\n",
    "        Fuse multi-scale features with learned attention weights.\n",
    "        multi_scale_features: list of features from different stages\n",
    "        \"\"\"\n",
    "        if len(multi_scale_features) != len(self.feature_dims):\n",
    "            raise ValueError(f\"Expected {len(self.feature_dims)} features, got {len(multi_scale_features)}\")\n",
    "        \n",
    "        # Get the target spatial size from the last (highest resolution) feature\n",
    "        target_size = multi_scale_features[-1].shape[2:]\n",
    "        \n",
    "        # Project and resize all features to the same dimension and spatial size\n",
    "        projected_features = []\n",
    "        for i, (feat, proj) in enumerate(zip(multi_scale_features, self.projections)):\n",
    "            projected = proj(feat)\n",
    "            if projected.shape[2:] != target_size:\n",
    "                projected = F.interpolate(projected, size=target_size, \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "            projected_features.append(projected)\n",
    "        \n",
    "        # Stack features and compute attention weights\n",
    "        stacked_features = torch.stack(projected_features, dim=2)  # [B, C, num_scales, H, W]\n",
    "        \n",
    "        # Use the mean of all features to compute attention\n",
    "        mean_feature = torch.mean(stacked_features, dim=2)  # [B, C, H, W]\n",
    "        attention_weights = self.scale_attention(mean_feature)  # [B, num_scales, 1, 1]\n",
    "        \n",
    "        # Apply attention weights and sum\n",
    "        attention_weights = attention_weights.unsqueeze(-1).unsqueeze(-1)  # [B, num_scales, 1, 1, 1]\n",
    "        weighted_features = stacked_features * attention_weights  # Broadcasting\n",
    "        fused_features = torch.sum(weighted_features, dim=2)  # [B, C, H, W]\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "# =========================\n",
    "# 5. SOLIDER Person Re-ID Model (Integrated)\n",
    "# =========================\n",
    "class SOLIDERPersonReIDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Person Re-ID model with SOLIDER adaptations integrated.\n",
    "    Maintains compatibility with your existing training framework.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feature_dim=2048, num_semantic_parts=3):\n",
    "        super(SOLIDERPersonReIDModel, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 and extract different stages\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Extract ResNet stages for multi-scale fusion\n",
    "        self.stage0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.stage1 = resnet.layer1  # 256 channels\n",
    "        self.stage2 = resnet.layer2  # 512 channels\n",
    "        self.stage3 = resnet.layer3  # 1024 channels\n",
    "        self.stage4 = resnet.layer4  # 2048 channels\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        self.multi_scale_fusion = MultiScaleFeatureFusion(\n",
    "            feature_dims=[256, 512, 1024, 2048], \n",
    "            output_dim=feature_dim\n",
    "        )\n",
    "        \n",
    "        # Spatial semantic clustering\n",
    "        self.semantic_clustering = SpatialSemanticClustering(\n",
    "            feature_dim=feature_dim, \n",
    "            num_semantic_parts=num_semantic_parts\n",
    "        )\n",
    "        \n",
    "        # Global pooling and classification head (same as your original)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn_neck = nn.BatchNorm1d(feature_dim)\n",
    "        self.bn_neck.bias.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes, bias=False)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_params()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters (same as your original model)\"\"\"\n",
    "        nn.init.kaiming_normal_(self.classifier.weight, mode='fan_out')\n",
    "        nn.init.constant_(self.bn_neck.weight, 1)\n",
    "        nn.init.constant_(self.bn_neck.bias, 0)\n",
    "    \n",
    "    def forward(self, x, lambda_val=0.5, return_semantic_loss=False, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass with SOLIDER semantic control.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [B, 3, H, W]\n",
    "            lambda_val: Semantic control parameter (0=appearance, 1=semantic)\n",
    "            return_semantic_loss: Whether to compute semantic loss\n",
    "            teacher_features: For student-teacher training (optional)\n",
    "        \"\"\"\n",
    "        # Extract multi-scale features\n",
    "        x0 = self.stage0(x)\n",
    "        x1 = self.stage1(x0)\n",
    "        x2 = self.stage2(x1)\n",
    "        x3 = self.stage3(x2)\n",
    "        x4 = self.stage4(x3)\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        fused_features = self.multi_scale_fusion([x1, x2, x3, x4])\n",
    "        \n",
    "        # Global pooling and feature extraction\n",
    "        pooled_features = self.global_pool(fused_features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        features = self.bn_neck(pooled_features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Semantic clustering (if training or requested)\n",
    "        semantic_output = None\n",
    "        if return_semantic_loss:\n",
    "            semantic_output = self.semantic_clustering(fused_features, teacher_features)\n",
    "        \n",
    "        if semantic_output is not None:\n",
    "            return features, logits, semantic_output\n",
    "        else:\n",
    "            return features, logits\n",
    "\n",
    "# =========================\n",
    "# 6. Integration with Your Existing Code\n",
    "# =========================\n",
    "def create_solider_model(num_classes):\n",
    "    \"\"\"\n",
    "    Factory function to create SOLIDER model that's compatible with your existing code.\n",
    "    \"\"\"\n",
    "    return SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "\n",
    "# =========================\n",
    "# Complete SOLIDERFIDITrainer with Fixed Method Signatures\n",
    "# =========================\n",
    "class SOLIDERFIDITrainer:\n",
    "    \"\"\"\n",
    "    Extended version of your FIDITrainer with SOLIDER semantic supervision.\n",
    "    Compatible with your existing training loop structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive', semantic_weight=0.5):\n",
    "        \n",
    "        # Initialize your existing FIDI trainer components\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)  # Your existing FIDI loss\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        self.semantic_weight = semantic_weight\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # SOLIDER-specific attributes\n",
    "        self.loss_history = {'fidi': [], 'ce': [], 'semantic': []}\n",
    "        self.best_mAP = 0.0\n",
    "        self.current_stage = 'fidi'  # 'fidi' or 'solider'\n",
    "        self.stage_switch_epoch = 100  # Switch to SOLIDER stage after this epoch\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"\n",
    "        Multiple loss weighting strategies based on training progress and loss magnitudes\n",
    "        (Copy your existing implementation from FIDITrainer)\n",
    "        \"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # More conservative approach - slower FIDI ramp-up, maintain CE importance\n",
    "            fidi_weight = min(0.8, progress * 1.5)  # Max 0.8, reaches it at 53% of training\n",
    "            cls_weight = max(0.8, 1.2 - progress)   # Min 0.8, gradual decrease\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Gradual transition with smooth curves\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))  # Sigmoid-like curve\n",
    "            cls_weight = 1.0 - 0.3 * progress  # Linear decrease to 0.7\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adaptive based on loss magnitudes (requires loss history)\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                # Calculate recent loss ratios\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                # Balance weights based on loss magnitudes\n",
    "                if recent_fidi > recent_ce * 2:  # FIDI much larger\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:  # CE much larger\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:  # Balanced\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                # Early training fallback\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            # Simple fixed weights\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original' - your current strategy\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        \"\"\"\n",
    "        Unified train_epoch method that handles both FIDI and SOLIDER stages.\n",
    "        Returns 8 values to match your training loop expectations.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        # Determine training stage\n",
    "        if epoch < self.stage_switch_epoch:\n",
    "            # Stage 1: FIDI training only\n",
    "            return self._train_epoch_stage1(dataloader, epoch, total_epochs)\n",
    "        else:\n",
    "            # Stage 2: SOLIDER training with semantic supervision\n",
    "            if epoch == self.stage_switch_epoch:\n",
    "                print(\"=\" * 50)\n",
    "                print(\"SWITCHING TO SOLIDER STAGE - Reducing learning rate\")\n",
    "                print(\"=\" * 50)\n",
    "                # Reduce learning rate for SOLIDER stage\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.1\n",
    "            \n",
    "            return self._train_epoch_stage2(dataloader, epoch, total_epochs)\n",
    "    \n",
    "    def _train_epoch_stage1(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"\n",
    "        Stage 1: Train with FIDI loss only (your existing method)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0  # Always 0 in stage 1\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        # Get dynamic weights for this epoch\n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Standard forward pass (no semantic loss in stage 1)\n",
    "            features, logits = self.model(images)\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Apply dynamic weighting (no semantic loss)\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store batch values\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = 0.0  # No semantic loss in stage 1\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'FIDI Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update loss history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def _train_epoch_stage2(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"\n",
    "        Stage 2: Train with FIDI + Semantic loss (SOLIDER adaptation)\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        # Sample lambda values following SOLIDER's binary distribution B(p=0.5)\n",
    "        lambda_vals = torch.bernoulli(torch.full((len(dataloader),), 0.5))\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Get current lambda value\n",
    "            current_lambda = lambda_vals[batch_idx].item()\n",
    "            \n",
    "            # Forward pass with semantic loss\n",
    "            if hasattr(self.model, 'module'):\n",
    "                features, logits, semantic_output = self.model.module(\n",
    "                    images, lambda_val=current_lambda, return_semantic_loss=True\n",
    "                )\n",
    "            else:\n",
    "                features, logits, semantic_output = self.model(\n",
    "                    images, lambda_val=current_lambda, return_semantic_loss=True\n",
    "                )\n",
    "            \n",
    "            # Compute losses\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            semantic_loss = semantic_output['semantic_loss']\n",
    "            \n",
    "            # Get dynamic weights\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "            \n",
    "            # Combined loss following SOLIDER's approach\n",
    "            loss = (fidi_weight * fidi_loss + \n",
    "                   cls_weight * ce_loss + \n",
    "                   self.semantic_weight * semantic_loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = semantic_loss.item()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'SOLIDER Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}, '\n",
    "                      f'Semantic={batch_semantic:.6f}×{self.semantic_weight:.2f}, '\n",
    "                      f'Lambda={current_lambda:.1f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update loss history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluation method - uses your existing implementation with SOLIDER adaptations\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        # Use optimal lambda for Person Re-ID evaluation (appearance-focused)\n",
    "        eval_lambda = 0.15\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                \n",
    "                # Forward pass with evaluation lambda\n",
    "                if hasattr(self.model, 'module'):\n",
    "                    if hasattr(self.model.module, 'forward') and 'lambda_val' in self.model.module.forward.__code__.co_varnames:\n",
    "                        features, _ = self.model.module(images, lambda_val=eval_lambda)[:2]\n",
    "                    else:\n",
    "                        features, _ = self.model(images)\n",
    "                else:\n",
    "                    if hasattr(self.model, 'forward') and 'lambda_val' in self.model.forward.__code__.co_varnames:\n",
    "                        features, _ = self.model(images, lambda_val=eval_lambda)[:2]\n",
    "                    else:\n",
    "                        features, _ = self.model(images)\n",
    "                \n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                \n",
    "                # Forward pass with evaluation lambda\n",
    "                if hasattr(self.model, 'module'):\n",
    "                    if hasattr(self.model.module, 'forward') and 'lambda_val' in self.model.module.forward.__code__.co_varnames:\n",
    "                        features, _ = self.model.module(images, lambda_val=eval_lambda)[:2]\n",
    "                    else:\n",
    "                        features, _ = self.model(images)\n",
    "                else:\n",
    "                    if hasattr(self.model, 'forward') and 'lambda_val' in self.model.forward.__code__.co_varnames:\n",
    "                        features, _ = self.model(images, lambda_val=eval_lambda)[:2]\n",
    "                    else:\n",
    "                        features, _ = self.model(images)\n",
    "                \n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        \"\"\"\n",
    "        Your existing CMC/mAP computation method - no changes needed\n",
    "        \"\"\"\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Fine-grained Difference-aware (FIDI) Pairwise Loss\n",
    "    Corrected implementation following the paper exactly\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.05, beta=0.5):\n",
    "        super(FIDILoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: tensor of shape (batch_size, feature_dim)\n",
    "            labels: tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = self.compute_pairwise_distances(features)\n",
    "        \n",
    "        # Compute ground truth binary relationship matrix K\n",
    "        labels = labels.view(-1, 1)\n",
    "        k_matrix = (labels == labels.T).float()  # 1 if same identity, 0 otherwise\n",
    "        \n",
    "        # Compute learned probability distribution U using exponential function\n",
    "        u_matrix = torch.exp(-self.beta * distances)\n",
    "        \n",
    "        # Compute D(U||K) + D(K||U)\n",
    "        d_u_k = self.compute_kl_divergence(u_matrix, k_matrix)\n",
    "        d_k_u = self.compute_kl_divergence(k_matrix, u_matrix)\n",
    "        \n",
    "        total_loss = d_u_k + d_k_u\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_pairwise_distances(self, features):\n",
    "        \"\"\"Compute Euclidean distances between all pairs of features\"\"\"\n",
    "        n = features.size(0)\n",
    "        # Expand features to compute all pairwise distances\n",
    "        features_1 = features.unsqueeze(1).expand(n, n, -1)\n",
    "        features_2 = features.unsqueeze(0).expand(n, n, -1)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distances = torch.sqrt(torch.sum((features_1 - features_2) ** 2, dim=2) + self.eps)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_kl_divergence(self, p_matrix, q_matrix):\n",
    "        \"\"\"\n",
    "        Compute KL divergence D(P||Q) following Equation (5) from the paper:\n",
    "        D(P||Q) = Σ p_ij * log(α * p_ij / ((α-1) * p_ij + q_ij))\n",
    "        \"\"\"\n",
    "        # Clamp to avoid numerical issues\n",
    "        p_matrix = torch.clamp(p_matrix, min=self.eps, max=1-self.eps)\n",
    "        q_matrix = torch.clamp(q_matrix, min=self.eps, max=1-self.eps)\n",
    "        \n",
    "        # Compute the denominator: (α-1) * p_ij + q_ij\n",
    "        denominator = (self.alpha - 1) * p_matrix + q_matrix\n",
    "        denominator = torch.clamp(denominator, min=self.eps)\n",
    "        \n",
    "        # Compute the fraction: α * p_ij / denominator\n",
    "        numerator = self.alpha * p_matrix\n",
    "        fraction = numerator / denominator\n",
    "        fraction = torch.clamp(fraction, min=self.eps)\n",
    "        \n",
    "        # Compute KL divergence: p_ij * log(fraction)\n",
    "        kl_div = p_matrix * torch.log(fraction)\n",
    "        \n",
    "        # Exclude diagonal elements (self-comparisons) and compute mean\n",
    "        mask = ~torch.eye(p_matrix.size(0), dtype=torch.bool, device=p_matrix.device)\n",
    "        kl_div = kl_div[mask].mean()\n",
    "        \n",
    "        return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e36ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Trainer Class\n",
    "# =========================\n",
    "class FIDITrainer:\n",
    "    \"\"\"\n",
    "    Improved Training framework for Person Re-ID with FIDI loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive'):\n",
    "        # Multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # For adaptive strategy\n",
    "        self.loss_history = {'fidi': [], 'ce': []}\n",
    "        self.best_mAP = 0.0\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"\n",
    "        Multiple loss weighting strategies based on training progress and loss magnitudes\n",
    "        \"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # More conservative approach - slower FIDI ramp-up, maintain CE importance\n",
    "            fidi_weight = min(0.8, progress * 1.5)  # Max 0.8, reaches it at 53% of training\n",
    "            cls_weight = max(0.8, 1.2 - progress)   # Min 0.8, gradual decrease\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Gradual transition with smooth curves\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))  # Sigmoid-like curve\n",
    "            cls_weight = 1.0 - 0.3 * progress  # Linear decrease to 0.7\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adaptive based on loss magnitudes (requires loss history)\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                # Calculate recent loss ratios\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                # Balance weights based on loss magnitudes\n",
    "                if recent_fidi > recent_ce * 2:  # FIDI much larger\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:  # CE much larger\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:  # Balanced\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                # Early training fallback\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            # Simple fixed weights\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original' - your current strategy\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        \n",
    "        # Get dynamic weights for this epoch\n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            features, logits = self.model(images)\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Apply dynamic weighting\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store all batch values\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_fidi_loss = total_fidi_loss / len(dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        std_loss = np.std(batch_losses)\n",
    "        \n",
    "        print(f'Epoch Summary: Avg Loss={avg_loss:.6f}, Min={min_loss:.6f}, Max={max_loss:.6f}, Std={std_loss:.6f}')\n",
    "        print(f'FIDI: Avg={avg_fidi_loss:.6f}, Min={min(batch_fidi_losses):.6f}, Max={max(batch_fidi_losses):.6f}')\n",
    "        print(f'CE: Avg={avg_ce_loss:.6f}, Min={min(batch_ce_losses):.6f}, Max={max(batch_ce_losses):.6f}')\n",
    "        \n",
    "        # Store loss history for adaptive strategy\n",
    "        self.loss_history['fidi'].append(avg_fidi_loss)\n",
    "        self.loss_history['ce'].append(avg_ce_loss)\n",
    "        if len(self.loss_history['fidi']) > 20:  # Keep only recent history\n",
    "            self.loss_history['fidi'].pop(0)\n",
    "            self.loss_history['ce'].pop(0)\n",
    "        \n",
    "        return avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n",
    "    \n",
    "    def train(self, train_dataloader, query_dataloader, gallery_dataloader, \n",
    "              num_epochs=120, eval_freq=10):\n",
    "        print(f\"Starting training with '{self.loss_strategy}' loss weighting strategy...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Get current weights for logging\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, num_epochs)\n",
    "            print(f'Loss weights - FIDI: {fidi_weight:.3f}, CE: {cls_weight:.3f}')\n",
    "            \n",
    "            avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses = self.train_epoch(\n",
    "                train_dataloader, epoch, num_epochs\n",
    "            )\n",
    "            print(f'Train Loss: {avg_loss:.4f}, FIDI Loss: {avg_fidi_loss:.4f}, '\n",
    "                  f'CE Loss: {avg_ce_loss:.4f}')\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                cmc, mAP = self.evaluate(query_dataloader, gallery_dataloader)\n",
    "                print(f'Rank-1: {cmc[0]:.4f}, Rank-5: {cmc[4]:.4f}, '\n",
    "                      f'Rank-10: {cmc[9]:.4f}, mAP: {mAP:.4f}')\n",
    "                \n",
    "                if mAP > self.best_mAP:\n",
    "                    self.best_mAP = mAP\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                        'mAP': mAP,\n",
    "                        'cmc': cmc,\n",
    "                        'loss_strategy': self.loss_strategy,\n",
    "                        'fidi_weight': fidi_weight,\n",
    "                        'cls_weight': cls_weight,\n",
    "                    }, 'best_model.pth')\n",
    "                    print(f'New best mAP: {self.best_mAP:.4f}')\n",
    "        \n",
    "        print(f'\\nTraining completed. Best mAP: {self.best_mAP:.4f}')\n",
    "        return self.best_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e9c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Tune-able Parameters / Config\n",
    "# =========================\n",
    "# PK Sampling parameters\n",
    "P = 16  # Number of persons per batch\n",
    "K = 4   # Number of images per person\n",
    "batch_size = P * K  # This will be 64 for optimal PK sampling\n",
    "\n",
    "num_epochs = 250\n",
    "device = [0, 1] if torch.cuda.device_count() > 1 else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alpha = 1.05\n",
    "beta = 0.5\n",
    "lr = 3.5e-4\n",
    "weight_decay = 5e-4\n",
    "num_workers = 8\n",
    "prefetch_factor = 4\n",
    "image_height = 256\n",
    "image_width = 128\n",
    "train_dir = os.path.join('Dataset', 'train')\n",
    "query_dir = os.path.join('Dataset', 'query')\n",
    "gallery_dir = os.path.join('Dataset', 'gallery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with PK sampling: P=16, K=4, batch=64\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7. Data Transforms & DataLoaders   ←  REPLACE THIS CELL\n",
    "# =========================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.Pad(10, padding_mode='edge'),      # add border for random crop\n",
    "    transforms.RandomCrop((image_height, image_width)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2, contrast=0.15, saturation=0.15, hue=0.1\n",
    "    ),                                            # lightweight colour aug\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(                     # ← paper-recommended\n",
    "        p=0.5, scale=(0.02, 0.4), ratio=(0.3, 3.3),\n",
    "        value='random'\n",
    "    ),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset   = PersonReIDTrainDataset(train_dir,  transform=train_transform)\n",
    "query_dataset   = PersonReIDTestDataset (query_dir,  transform=test_transform)\n",
    "gallery_dataset = PersonReIDTestDataset (gallery_dir, transform=test_transform)\n",
    "\n",
    "num_classes = len(train_dataset.label_map)\n",
    "\n",
    "pk_sampler = PKSampler(train_dataset, P=P, K=K)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=pk_sampler,          # PK sampling\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,   batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor\n",
    ")\n",
    "gallery_loader = DataLoader(\n",
    "    gallery_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "print(f\"Training with PK sampling: P={P}, K={K}, batch={P*K}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3092619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. Model & Trainer Initialization\n",
    "# =========================\n",
    "#model = PersonReIDModel(num_classes=num_classes)\n",
    "model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "\n",
    "# =========================\n",
    "# 8b. Export Model to ONNX for Netron (before training)\n",
    "# =========================\n",
    "# import torch\n",
    "# onnx_path = \"person_reid_model.onnx\"\n",
    "# # Determine device for dummy input\n",
    "# if isinstance(device, (list, tuple)):\n",
    "#     dummy_device = f\"cuda:{device[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "# else:\n",
    "#     dummy_device = device\n",
    "# # Create a sample input\n",
    "# model = model.to(dummy_device)\n",
    "# sample_input = torch.randn(1, 3, image_height, image_width, device=dummy_device)\n",
    "# # If using DataParallel, get the underlying model\n",
    "# export_model = model.module if hasattr(model, \"module\") else model\n",
    "# # Export the model\n",
    "# export_model.eval()\n",
    "# torch.onnx.export(\n",
    "#     export_model,\n",
    "#     sample_input,\n",
    "#     onnx_path,\n",
    "#     export_params=True,\n",
    "#     opset_version=11,\n",
    "#     do_constant_folding=True,\n",
    "#     input_names=['input'],\n",
    "#     output_names=['features', 'logits'],\n",
    "#     dynamic_axes={'input': {0: 'batch_size'}, 'features': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}\n",
    "# )\n",
    "# print(f\"Model exported to {onnx_path}. You can now open it in Netron for architecture visualization.\")\n",
    "\n",
    "print(\"ONNX export disabled for SOLIDER model - proceeding with training...\")\n",
    "\n",
    "# trainer = FIDITrainer(\n",
    "#     model=model,\n",
    "#     num_classes=num_classes,\n",
    "#     device=device,\n",
    "#     alpha=alpha,\n",
    "#     beta=beta,\n",
    "#     lr=lr,\n",
    "#     weight_decay=weight_decay,\n",
    "#     loss_strategy='progressive'\n",
    "# )\n",
    "\n",
    "trainer = SOLIDERFIDITrainer(\n",
    "    model=model,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    loss_strategy='progressive',\n",
    "    semantic_weight=0.5  # Weight for semantic loss\n",
    ")\n",
    "\n",
    "# # For challenging datasets (CUHK03)\n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='conservative')\n",
    "\n",
    "# # For easier datasets (Market1501) \n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='progressive')\n",
    "\n",
    "# # For experimental/research purposes\n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df92d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting SOLIDER CNN model to ONNX...\n",
      "Exporting core SOLIDER architecture...\n",
      "✓ SOLIDER core architecture exported to: onnx_models/solider_core_architecture.onnx\n",
      "\n",
      "📊 SOLIDER Model Statistics:\n",
      "   • Total parameters: 36,610,120\n",
      "   • Trainable parameters: 36,608,072\n",
      "   • Input shape: (1, 3, 256, 128)\n",
      "   • Number of classes: 751\n",
      "\n",
      "🔍 Netron Visualization:\n",
      "   1. Open Netron (https://netron.app/)\n",
      "   2. Load the exported ONNX file: onnx_models/solider_core_architecture.onnx\n",
      "\n",
      "💡 Key SOLIDER Components to Look For:\n",
      "   • Multi-scale feature fusion (stages 1-4)\n",
      "   • Spatial semantic clustering\n",
      "   • Semantic controller modules\n",
      "   • ResNet backbone with SOLIDER blocks\n",
      "\n",
      "✅ SOLIDER model successfully exported to ONNX!\n",
      "   You can now visualize it in Netron!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ONNX Export for SOLIDER Model (Netron Visualization) - SIMPLIFIED CORE\n",
    "# =========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def export_solider_model_to_onnx():\n",
    "    \"\"\"\n",
    "    Export the SOLIDER CNN model to ONNX format for Netron visualization\n",
    "    \"\"\"\n",
    "    print(\"Exporting SOLIDER CNN model to ONNX...\")\n",
    "    \n",
    "    # Create SOLIDER model instance\n",
    "    solider_model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "    \n",
    "    # Determine device for dummy input\n",
    "    if isinstance(device, (list, tuple)):\n",
    "        dummy_device = f\"cuda:{device[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "    else:\n",
    "        dummy_device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Move model to device\n",
    "    solider_model = solider_model.to(dummy_device)\n",
    "    solider_model.eval()\n",
    "    \n",
    "    # Create sample input tensor\n",
    "    batch_size = 1\n",
    "    sample_input = torch.randn(batch_size, 3, image_height, image_width, device=dummy_device)\n",
    "    \n",
    "    # Define ONNX export paths\n",
    "    onnx_dir = \"onnx_models\"\n",
    "    os.makedirs(onnx_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a wrapper that exports just the core backbone without the final layers\n",
    "    class SOLIDERCoreWrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "            \n",
    "            # Extract the core backbone (stages 0-4)\n",
    "            self.stage0 = model.stage0\n",
    "            self.stage1 = model.stage1\n",
    "            self.stage2 = model.stage2\n",
    "            self.stage3 = model.stage3\n",
    "            self.stage4 = model.stage4\n",
    "            \n",
    "            # Extract multi-scale fusion\n",
    "            self.multi_scale_fusion = model.multi_scale_fusion\n",
    "            \n",
    "            # Extract semantic clustering (without final layers)\n",
    "            self.semantic_clustering = model.semantic_clustering\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Forward through stages\n",
    "            stage0_out = self.stage0(x)\n",
    "            stage1_out = self.stage1(stage0_out)\n",
    "            stage2_out = self.stage2(stage1_out)\n",
    "            stage3_out = self.stage3(stage2_out)\n",
    "            stage4_out = self.stage4(stage3_out)\n",
    "            \n",
    "            # Multi-scale fusion\n",
    "            fused_features = self.multi_scale_fusion([stage1_out, stage2_out, stage3_out, stage4_out])\n",
    "            \n",
    "            # Global pooling\n",
    "            pooled_features = torch.nn.functional.adaptive_avg_pool2d(fused_features, (1, 1))\n",
    "            pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "            \n",
    "            # Return intermediate features for visualization\n",
    "            return pooled_features, stage4_out\n",
    "    \n",
    "    # Export core architecture\n",
    "    core_onnx_path = os.path.join(onnx_dir, \"solider_core_architecture.onnx\")\n",
    "    core_wrapper = SOLIDERCoreWrapper(solider_model)\n",
    "    \n",
    "    try:\n",
    "        print(\"Exporting core SOLIDER architecture...\")\n",
    "        torch.onnx.export(\n",
    "            core_wrapper,\n",
    "            sample_input,\n",
    "            core_onnx_path,\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input_image'],\n",
    "            output_names=['pooled_features', 'stage4_features'],\n",
    "            dynamic_axes={\n",
    "                'input_image': {0: 'batch_size'}, \n",
    "                'pooled_features': {0: 'batch_size'}, \n",
    "                'stage4_features': {0: 'batch_size'}\n",
    "            },\n",
    "            verbose=False\n",
    "        )\n",
    "        print(f\"✓ SOLIDER core architecture exported to: {core_onnx_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export core architecture: {str(e)}\")\n",
    "        \n",
    "        # Try an even simpler approach - just the backbone stages\n",
    "        print(\"Trying simplified backbone export...\")\n",
    "        \n",
    "        class SOLIDERBackboneWrapper(nn.Module):\n",
    "            def __init__(self, model):\n",
    "                super().__init__()\n",
    "                self.stage0 = model.stage0\n",
    "                self.stage1 = model.stage1\n",
    "                self.stage2 = model.stage2\n",
    "                self.stage3 = model.stage3\n",
    "                self.stage4 = model.stage4\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.stage0(x)\n",
    "                x = self.stage1(x)\n",
    "                x = self.stage2(x)\n",
    "                x = self.stage3(x)\n",
    "                x = self.stage4(x)\n",
    "                return x\n",
    "        \n",
    "        backbone_onnx_path = os.path.join(onnx_dir, \"solider_backbone_stages.onnx\")\n",
    "        backbone_wrapper = SOLIDERBackboneWrapper(solider_model)\n",
    "        \n",
    "        try:\n",
    "            torch.onnx.export(\n",
    "                backbone_wrapper,\n",
    "                sample_input,\n",
    "                backbone_onnx_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input_image'],\n",
    "                output_names=['backbone_output'],\n",
    "                dynamic_axes={\n",
    "                    'input_image': {0: 'batch_size'}, \n",
    "                    'backbone_output': {0: 'batch_size'}\n",
    "                },\n",
    "                verbose=False\n",
    "            )\n",
    "            print(f\"✓ SOLIDER backbone stages exported to: {backbone_onnx_path}\")\n",
    "            core_onnx_path = backbone_onnx_path\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Failed to export backbone stages: {str(e2)}\")\n",
    "            return None\n",
    "    \n",
    "    # Print model statistics\n",
    "    total_params = sum(p.numel() for p in solider_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in solider_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\n📊 SOLIDER Model Statistics:\")\n",
    "    print(f\"   • Total parameters: {total_params:,}\")\n",
    "    print(f\"   • Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   • Input shape: {tuple(sample_input.shape)}\")\n",
    "    print(f\"   • Number of classes: {num_classes}\")\n",
    "    \n",
    "    print(f\"\\n🔍 Netron Visualization:\")\n",
    "    print(f\"   1. Open Netron (https://netron.app/)\")\n",
    "    print(f\"   2. Load the exported ONNX file: {core_onnx_path}\")\n",
    "    \n",
    "    print(f\"\\n💡 Key SOLIDER Components to Look For:\")\n",
    "    print(f\"   • Multi-scale feature fusion (stages 1-4)\")\n",
    "    print(f\"   • Spatial semantic clustering\")\n",
    "    print(f\"   • Semantic controller modules\")\n",
    "    print(f\"   • ResNet backbone with SOLIDER blocks\")\n",
    "    \n",
    "    return {'core': core_onnx_path}\n",
    "\n",
    "# Execute the export\n",
    "try:\n",
    "    exported_models = export_solider_model_to_onnx()\n",
    "    if exported_models:\n",
    "        print(\"\\n✅ SOLIDER model successfully exported to ONNX!\")\n",
    "        print(\"   You can now visualize it in Netron!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Failed to export SOLIDER model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during export: {str(e)}\")\n",
    "    print(\"Make sure the SOLIDERPersonReIDModel class has been defined and all dependencies are imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5e6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m log_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m log_print(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m avg_loss, avg_fidi_loss, avg_ce_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_epoch(train_loader, epoch, num_epochs_to_run)\n\u001b[1;32m     54\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m     55\u001b[0m fidi_losses\u001b[38;5;241m.\u001b[39mappend(avg_fidi_loss)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Training Loop with Live Loss & Accuracy Plot\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up logging to file - ONLY ONCE at the beginning\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "log_file = open(log_filename, 'w')\n",
    "\n",
    "# Create a custom print function that writes to both console and file\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    print(*args, **kwargs, file=log_file)\n",
    "    log_file.flush()  # Ensure immediate writing\n",
    "\n",
    "# Log training start\n",
    "log_print(f\"Training started at: {datetime.now()}\")\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(f\"PK sampling: P={P}, K={K}, batch_size={P*K}\")\n",
    "log_print(f\"Number of classes: {num_classes}\")\n",
    "log_print(\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "fidi_losses = []\n",
    "ce_losses = []\n",
    "semantic_losses = []  # ADD THIS FOR SOLIDER\n",
    "epochs = []\n",
    "rank1s = []\n",
    "maps = []\n",
    "eval_epochs = []\n",
    "\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(\"SOLIDER model and trainer initialized successfully!\")\n",
    "log_print(\"Starting training...\")\n",
    "\n",
    "num_epochs_to_run = num_epochs  # You can override this for shorter runs\n",
    "eval_freq = 10  # You can set this in your config cell if you want\n",
    "\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for epoch in range(num_epochs_to_run):\n",
    "    log_print(f'\\nEpoch {epoch+1}/{num_epochs_to_run}')\n",
    "    log_print('-' * 50)\n",
    "    avg_loss, avg_fidi_loss, avg_ce_loss, avg_semantic_loss, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses = trainer.train_epoch(train_loader, epoch, num_epochs_to_run)\n",
    "    train_losses.append(avg_loss)\n",
    "    fidi_losses.append(avg_fidi_loss)\n",
    "    ce_losses.append(avg_ce_loss)\n",
    "    semantic_losses.append(avg_semantic_loss)  # ADD THIS\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # Evaluate and collect accuracy/mAP\n",
    "    if (epoch + 1) % eval_freq == 0 or (epoch + 1) == num_epochs_to_run:\n",
    "        log_print(\"Evaluating...\")\n",
    "        cmc, mAP = trainer.evaluate(query_loader, gallery_loader)\n",
    "        rank1 = float(cmc[0].item())\n",
    "        rank1s.append(rank1)\n",
    "        maps.append(float(mAP))\n",
    "        eval_epochs.append(epoch + 1)\n",
    "        log_print(f'Rank-1: {rank1:.4f}, mAP: {mAP:.4f}')\n",
    "\n",
    "    # Live plot\n",
    "    clear_output(wait=True)\n",
    "    ax1.clear()\n",
    "    ax1.plot(epochs, train_losses, label='Total Loss')\n",
    "    ax1.plot(epochs, fidi_losses, label='FIDI Loss')\n",
    "    ax1.plot(epochs, ce_losses, label='CE Loss')\n",
    "    ax1.plot(epochs, semantic_losses, label='Semantic Loss')  # ADD THIS\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.plot(eval_epochs, rank1s, label='Rank-1 Accuracy')\n",
    "    ax2.plot(eval_epochs, maps, label='mAP')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Validation: Rank-1 & mAP')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    display(fig)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    trainer.scheduler.step()\n",
    "\n",
    "    # Save TorchScript model every 5 epochs\n",
    "    # Save both TorchScript (.pt) and PyTorch (.pth) models every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Create weights directory if it doesn't exist\n",
    "        os.makedirs(\"weights\", exist_ok=True)\n",
    "        \n",
    "        # Save PyTorch state dict (.pth file)\n",
    "        pth_path = f\"weights/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': trainer.model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict()\n",
    "        }, pth_path)\n",
    "        \n",
    "        # Save TorchScript model (.pt file) - also in weights folder\n",
    "        model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "        scripted = torch.jit.script(model_to_save)\n",
    "        script_path = f\"weights/checkpoint_epoch_{epoch+1}.pt\"\n",
    "        scripted.save(script_path)\n",
    "        \n",
    "        log_print(f\"Models saved - PyTorch: {pth_path}, TorchScript: {script_path}\")\n",
    "\n",
    "# Close the log file at the end\n",
    "log_file.close()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "log_print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
