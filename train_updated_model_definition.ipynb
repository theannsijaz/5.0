{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3945b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class PKSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Fixed PK Sampler for Person Re-ID: P persons × K images per person\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, P=16, K=4):\n",
    "        self.data_source = data_source\n",
    "        self.P = P  # Number of persons per batch\n",
    "        self.K = K  # Number of images per person\n",
    "        \n",
    "        # Group samples by person ID\n",
    "        self.pid_to_indices = defaultdict(list)\n",
    "        for idx, (_, pid) in enumerate(data_source.samples):\n",
    "            self.pid_to_indices[pid].append(idx)\n",
    "        \n",
    "        # Filter out persons with less than K images\n",
    "        self.valid_pids = [pid for pid, indices in self.pid_to_indices.items() \n",
    "                          if len(indices) >= self.K]\n",
    "        \n",
    "        if len(self.valid_pids) < self.P:\n",
    "            raise ValueError(f\"Not enough persons with at least {self.K} images. \"\n",
    "                           f\"Found {len(self.valid_pids)}, need {self.P}\")\n",
    "        \n",
    "        # Calculate total number of samples we'll generate\n",
    "        self.num_batches = len(self.valid_pids) // self.P\n",
    "        self.total_size = self.num_batches * self.P * self.K\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Fixed iterator that yields individual indices, not batches\"\"\"\n",
    "        # Shuffle valid PIDs for each epoch\n",
    "        shuffled_pids = self.valid_pids.copy()\n",
    "        random.shuffle(shuffled_pids)\n",
    "        \n",
    "        # Generate all indices for this epoch\n",
    "        all_indices = []\n",
    "        \n",
    "        for batch_start in range(0, len(shuffled_pids) - self.P + 1, self.P):\n",
    "            # Select P persons for this batch\n",
    "            batch_pids = shuffled_pids[batch_start:batch_start + self.P]\n",
    "            \n",
    "            batch_indices = []\n",
    "            for pid in batch_pids:\n",
    "                # Randomly select K images for this person\n",
    "                available_indices = self.pid_to_indices[pid]\n",
    "                if len(available_indices) >= self.K:\n",
    "                    selected_indices = random.sample(available_indices, self.K)\n",
    "                    batch_indices.extend(selected_indices)\n",
    "            \n",
    "            # Shuffle within batch to avoid ordering bias\n",
    "            random.shuffle(batch_indices)\n",
    "            all_indices.extend(batch_indices)\n",
    "        \n",
    "        # Yield individual indices\n",
    "        for idx in all_indices:\n",
    "            yield idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "class PersonReIDTrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training set: expects structure Dataset/train/<pid>/*.jpg\n",
    "    Returns (image, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label)\n",
    "        self.label_map = {}  # pid (str) -> label (int)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        pids = sorted(os.listdir(self.root_dir))\n",
    "        self.label_map = {pid: idx for idx, pid in enumerate(pids)}\n",
    "        for pid in pids:\n",
    "            pid_dir = os.path.join(self.root_dir, pid)\n",
    "            if not os.path.isdir(pid_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(pid_dir):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.samples.append((os.path.join(pid_dir, fname), self.label_map[pid]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load image {img_path}: {e}\")\n",
    "            # Return a dummy image\n",
    "            dummy_img = torch.zeros(3, 256, 128)\n",
    "            return dummy_img, label\n",
    "\n",
    "class PersonReIDTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for query/gallery set: expects structure Dataset/query/*.jpg or Dataset/gallery/*.jpg\n",
    "    Returns (image, label, cam_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label, cam_id)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        for fname in os.listdir(self.dir_path):\n",
    "            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                try:\n",
    "                    # Example: 0001_c1s1_001051_00.jpg\n",
    "                    parts = fname.split('_')\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    label = int(parts[0])\n",
    "                    cam_id = int(parts[1][1])  # e.g., c1 -> 1\n",
    "                    self.samples.append((os.path.join(self.dir_path, fname), label, cam_id))\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    print(f\"Warning: Skipping file {fname} due to naming format: {e}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, cam_id = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label, cam_id\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load image {img_path}: {e}\")\n",
    "            # Return a dummy image\n",
    "            dummy_img = torch.zeros(3, 256, 128)\n",
    "            return dummy_img, label, cam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ff540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatialSemanticClustering(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved spatial-level semantic clustering for CNN feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, num_semantic_parts=3, momentum=0.99):\n",
    "        super(SpatialSemanticClustering, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_semantic_parts = num_semantic_parts\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Semantic head with dropout for robustness\n",
    "        self.semantic_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 2),\n",
    "            nn.BatchNorm1d(feature_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(feature_dim // 2, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(feature_dim // 4, num_semantic_parts + 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def spatial_semantic_labeling(self, feature_maps):\n",
    "        \"\"\"Generate spatial semantic labels based on human priors.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        # Create spatial coordinate grids\n",
    "        y_coords = torch.linspace(0, 1, H, device=device).view(H, 1).expand(H, W)\n",
    "        \n",
    "        # Human prior: spatial semantic assignment\n",
    "        semantic_labels = torch.zeros(H, W, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Upper body: top 40%\n",
    "        upper_mask = y_coords < 0.4\n",
    "        semantic_labels[upper_mask] = 0\n",
    "        \n",
    "        # Lower body: middle 40%\n",
    "        middle_mask = (y_coords >= 0.4) & (y_coords < 0.8)\n",
    "        semantic_labels[middle_mask] = 1\n",
    "        \n",
    "        # Shoes: bottom 20%\n",
    "        lower_mask = y_coords >= 0.8\n",
    "        semantic_labels[lower_mask] = 2\n",
    "        \n",
    "        return semantic_labels\n",
    "    \n",
    "    def foreground_background_clustering(self, feature_maps):\n",
    "        \"\"\"Separate foreground and background with improved stability.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        \n",
    "        # Calculate feature magnitude\n",
    "        feature_magnitude = torch.norm(feature_maps, dim=1, p=2)  # [B, H, W]\n",
    "        \n",
    "        # Adaptive threshold with clamping for stability\n",
    "        batch_mean = feature_magnitude.mean(dim=(1, 2), keepdim=True)\n",
    "        batch_std = feature_magnitude.std(dim=(1, 2), keepdim=True)\n",
    "        \n",
    "        # Clamp std to avoid division by zero\n",
    "        batch_std = torch.clamp(batch_std, min=1e-6)\n",
    "        fg_threshold = batch_mean + 0.5 * batch_std\n",
    "        \n",
    "        # Create foreground mask\n",
    "        fg_mask = feature_magnitude > fg_threshold\n",
    "        \n",
    "        return fg_mask\n",
    "    \n",
    "    def forward(self, student_features, teacher_features=None):\n",
    "        \"\"\"Forward pass with improved error handling.\"\"\"\n",
    "        B, C, H, W = student_features.shape\n",
    "        device = student_features.device\n",
    "        \n",
    "        try:\n",
    "            # Use teacher features if available\n",
    "            clustering_features = teacher_features if teacher_features is not None else student_features\n",
    "            \n",
    "            # Generate pseudo semantic labels\n",
    "            fg_mask = self.foreground_background_clustering(clustering_features)\n",
    "            spatial_labels = self.spatial_semantic_labeling(clustering_features)\n",
    "            \n",
    "            # Combine foreground mask with spatial labels\n",
    "            pseudo_labels = torch.full((B, H, W), self.num_semantic_parts, \n",
    "                                     dtype=torch.long, device=device)\n",
    "            \n",
    "            for b in range(B):\n",
    "                fg_positions = fg_mask[b]\n",
    "                if torch.any(fg_positions):  # Check if any foreground pixels exist\n",
    "                    pseudo_labels[b][fg_positions] = spatial_labels[fg_positions]\n",
    "            \n",
    "            # Flatten for classification\n",
    "            student_flat = student_features.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "            labels_flat = pseudo_labels.reshape(-1)\n",
    "            \n",
    "            # Semantic classification with better error handling\n",
    "            if student_flat.size(0) > 0:\n",
    "                semantic_logits = self.semantic_head(student_flat)\n",
    "                # Use label smoothing for better training stability\n",
    "                semantic_loss = F.cross_entropy(semantic_logits, labels_flat, \n",
    "                                               reduction='mean', label_smoothing=0.1)\n",
    "            else:\n",
    "                semantic_logits = torch.zeros(B*H*W, self.num_semantic_parts + 1, device=device)\n",
    "                semantic_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            return {\n",
    "                'semantic_loss': semantic_loss,\n",
    "                'pseudo_labels': pseudo_labels,\n",
    "                'foreground_mask': fg_mask,\n",
    "                'semantic_logits': semantic_logits.view(B, H, W, -1)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Semantic clustering failed: {e}\")\n",
    "            # Safe fallback\n",
    "            return {\n",
    "                'semantic_loss': torch.tensor(0.0, device=device, requires_grad=True),\n",
    "                'pseudo_labels': torch.zeros(B, H, W, dtype=torch.long, device=device),\n",
    "                'foreground_mask': torch.ones(B, H, W, dtype=torch.bool, device=device),\n",
    "                'semantic_logits': torch.zeros(B, H, W, self.num_semantic_parts + 1, device=device)\n",
    "            }\n",
    "\n",
    "class SemanticController(nn.Module):\n",
    "    \"\"\"Improved semantic controller with better parameter handling.\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(SemanticController, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Lambda encoding networks\n",
    "        self.weight_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.bias_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, feature_maps, lambda_val=0.5):\n",
    "        \"\"\"Apply semantic control with robust parameter handling.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        try:\n",
    "            # Robust lambda value handling\n",
    "            if isinstance(lambda_val, (int, float)):\n",
    "                lambda_tensor = torch.tensor([[float(lambda_val)]], device=device, dtype=torch.float32)\n",
    "            elif isinstance(lambda_val, torch.Tensor):\n",
    "                if lambda_val.dim() == 0:\n",
    "                    lambda_tensor = lambda_val.unsqueeze(0).unsqueeze(0).to(device).float()\n",
    "                elif lambda_val.dim() == 1:\n",
    "                    lambda_tensor = lambda_val.unsqueeze(1).to(device).float()\n",
    "                else:\n",
    "                    lambda_tensor = lambda_val.to(device).float()\n",
    "            else:\n",
    "                lambda_tensor = torch.tensor([[0.5]], device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Ensure we have the right shape [1, 1]\n",
    "            if lambda_tensor.numel() == 0:\n",
    "                lambda_tensor = torch.tensor([[0.5]], device=device, dtype=torch.float32)\n",
    "            elif lambda_tensor.shape != (1, 1):\n",
    "                lambda_tensor = lambda_tensor.view(1, 1)\n",
    "            \n",
    "            # Encode lambda into weights and biases\n",
    "            weights = self.weight_encoder(lambda_tensor)  # [1, C]\n",
    "            biases = self.bias_encoder(lambda_tensor)     # [1, C]\n",
    "            \n",
    "            # Expand for broadcasting\n",
    "            weights = weights.view(1, C, 1, 1).expand(B, C, H, W)\n",
    "            biases = biases.view(1, C, 1, 1).expand(B, C, H, W)\n",
    "            \n",
    "            # Apply semantic control\n",
    "            controlled_features = weights * feature_maps + biases\n",
    "            \n",
    "            return controlled_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Semantic control failed: {e}, returning original features\")\n",
    "            return feature_maps\n",
    "\n",
    "class SOLIDERCNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet block integrated with SOLIDER semantic control.\n",
    "    Maintains ResNet structure while adding semantic controllability.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(SOLIDERCNNBlock, self).__init__()\n",
    "        \n",
    "        # Standard ResNet block components\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # SOLIDER semantic controller\n",
    "        self.semantic_controller = SemanticController(out_channels)\n",
    "        \n",
    "    def forward(self, x, lambda_val=0.5):\n",
    "        identity = x\n",
    "        \n",
    "        # Standard ResNet forward pass\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply downsampling if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Apply semantic control before adding residual\n",
    "        out = self.semantic_controller(out, lambda_val)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed multi-scale feature fusion with proper dimension handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dims=None, output_dim=2048):\n",
    "        super(MultiScaleFeatureFusion, self).__init__()\n",
    "        \n",
    "        # Default ResNet50 dimensions if not provided\n",
    "        if feature_dims is None:\n",
    "            feature_dims = [256, 512, 1024, 2048]\n",
    "        \n",
    "        self.feature_dims = feature_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Projection layers to align dimensions\n",
    "        self.projections = nn.ModuleList()\n",
    "        for dim in feature_dims:\n",
    "            if dim != output_dim:\n",
    "                self.projections.append(nn.Sequential(\n",
    "                    nn.Conv2d(dim, output_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(output_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                ))\n",
    "            else:\n",
    "                # Identity projection for same dimension\n",
    "                self.projections.append(nn.Identity())\n",
    "        \n",
    "        # Attention mechanism for scale weighting\n",
    "        self.scale_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(output_dim, output_dim // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_dim // 4, len(feature_dims), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, multi_scale_features):\n",
    "        \"\"\"Fuse multi-scale features with proper error handling.\"\"\"\n",
    "        if len(multi_scale_features) != len(self.feature_dims):\n",
    "            print(f\"Warning: Expected {len(self.feature_dims)} features, got {len(multi_scale_features)}\")\n",
    "            # Use only the available features\n",
    "            multi_scale_features = multi_scale_features[:len(self.feature_dims)]\n",
    "            if len(multi_scale_features) == 0:\n",
    "                # Fallback: return zeros\n",
    "                B = 1\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                return torch.zeros(B, self.output_dim, 8, 4, device=device)\n",
    "        \n",
    "        # Get target size from the highest resolution feature (last one)\n",
    "        target_size = multi_scale_features[-1].shape[2:]\n",
    "        \n",
    "        # Project and resize features\n",
    "        projected_features = []\n",
    "        for i, (feat, proj) in enumerate(zip(multi_scale_features, self.projections)):\n",
    "            try:\n",
    "                # Apply projection\n",
    "                projected = proj(feat)\n",
    "                \n",
    "                # Resize if needed\n",
    "                if projected.shape[2:] != target_size:\n",
    "                    projected = F.interpolate(\n",
    "                        projected, size=target_size, \n",
    "                        mode='bilinear', align_corners=False\n",
    "                    )\n",
    "                \n",
    "                projected_features.append(projected)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process feature {i}: {e}\")\n",
    "                # Skip this feature\n",
    "                continue\n",
    "        \n",
    "        if not projected_features:\n",
    "            # Fallback if all projections failed\n",
    "            B, _, H, W = multi_scale_features[-1].shape\n",
    "            device = multi_scale_features[-1].device\n",
    "            return torch.zeros(B, self.output_dim, H, W, device=device)\n",
    "        \n",
    "        # Stack features for attention computation\n",
    "        stacked_features = torch.stack(projected_features, dim=1)  # [B, num_scales, C, H, W]\n",
    "        B, num_scales, C, H, W = stacked_features.shape\n",
    "        \n",
    "        # Compute attention weights using mean feature\n",
    "        mean_feature = torch.mean(stacked_features, dim=1)  # [B, C, H, W]\n",
    "        attention_weights = self.scale_attention(mean_feature)  # [B, num_scales, 1, 1]\n",
    "        \n",
    "        # Apply attention and fuse\n",
    "        attention_weights = attention_weights.unsqueeze(2)  # [B, num_scales, 1, 1, 1]\n",
    "        weighted_features = stacked_features * attention_weights\n",
    "        fused_features = torch.sum(weighted_features, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "class SOLIDERPersonReIDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed SOLIDER Person Re-ID model with improved stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feature_dim=2048, num_semantic_parts=3):\n",
    "        super(SOLIDERPersonReIDModel, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 backbone\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Extract stages\n",
    "        self.stage0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.stage1 = resnet.layer1  # 256 channels\n",
    "        self.stage2 = resnet.layer2  # 512 channels  \n",
    "        self.stage3 = resnet.layer3  # 1024 channels\n",
    "        self.stage4 = resnet.layer4  # 2048 channels\n",
    "        \n",
    "        # Multi-scale fusion with correct dimensions\n",
    "        self.multi_scale_fusion = MultiScaleFeatureFusion(\n",
    "            feature_dims=[256, 512, 1024, 2048],\n",
    "            output_dim=feature_dim\n",
    "        )\n",
    "        \n",
    "        # Semantic clustering\n",
    "        self.semantic_clustering = SpatialSemanticClustering(\n",
    "            feature_dim=feature_dim,\n",
    "            num_semantic_parts=num_semantic_parts\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn_neck = nn.BatchNorm1d(feature_dim)\n",
    "        self.bn_neck.bias.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes, bias=False)\n",
    "        \n",
    "        self._init_params()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        nn.init.kaiming_normal_(self.classifier.weight, mode='fan_out')\n",
    "        nn.init.constant_(self.bn_neck.weight, 1)\n",
    "        nn.init.constant_(self.bn_neck.bias, 0)\n",
    "    \n",
    "    def forward(self, x, lambda_val=0.5, return_semantic_loss=False, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass with consistent output handling.\n",
    "        \"\"\"\n",
    "        # Extract multi-scale features\n",
    "        x0 = self.stage0(x)\n",
    "        x1 = self.stage1(x0)\n",
    "        x2 = self.stage2(x1)  \n",
    "        x3 = self.stage3(x2)\n",
    "        x4 = self.stage4(x3)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        fused_features = self.multi_scale_fusion([x1, x2, x3, x4])\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        pooled_features = self.global_pool(fused_features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        features = self.bn_neck(pooled_features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Return based on request\n",
    "        if return_semantic_loss:\n",
    "            try:\n",
    "                semantic_output = self.semantic_clustering(fused_features, teacher_features)\n",
    "                return features, logits, semantic_output\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Semantic clustering failed: {e}\")\n",
    "                # Return with zero semantic loss\n",
    "                device = features.device\n",
    "                semantic_output = {\n",
    "                    'semantic_loss': torch.tensor(0.0, device=device, requires_grad=True)\n",
    "                }\n",
    "                return features, logits, semantic_output\n",
    "        else:\n",
    "            return features, logits\n",
    "\n",
    "def create_solider_model(num_classes):\n",
    "    \"\"\"Factory function to create SOLIDER model.\"\"\"\n",
    "    return SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "\n",
    "\n",
    "class SOLIDERFIDITrainer:\n",
    "    \"\"\"\n",
    "    Fixed SOLIDER-enhanced FIDI trainer with better error handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive', semantic_weight=0.5):\n",
    "        \n",
    "        # Device setup with multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "            self.is_parallel = True\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "            self.is_parallel = False\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        self.semantic_weight = semantic_weight\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.loss_history = {'fidi': [], 'ce': [], 'semantic': []}\n",
    "        self.best_mAP = 0.0\n",
    "        self.stage_switch_epoch = 100  # Switch to SOLIDER stage after this epoch\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Get the actual model (handle DataParallel wrapper)\"\"\"\n",
    "        return self.model.module if self.is_parallel else self.model\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"Get dynamic loss weights based on training progress.\"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            fidi_weight = min(0.8, progress * 1.5)\n",
    "            cls_weight = max(0.8, 1.2 - progress)\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))\n",
    "            cls_weight = 1.0 - 0.3 * progress\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                if recent_fidi > recent_ce * 2:\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original'\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        \"\"\"\n",
    "        Fixed train_epoch method that handles both FIDI and SOLIDER stages.\n",
    "        \"\"\"\n",
    "        # Determine training stage\n",
    "        if epoch < self.stage_switch_epoch:\n",
    "            return self._train_epoch_stage1(dataloader, epoch, total_epochs)\n",
    "        else:\n",
    "            if epoch == self.stage_switch_epoch:\n",
    "                print(\"=\" * 50)\n",
    "                print(\"SWITCHING TO SOLIDER STAGE\")\n",
    "                print(\"=\" * 50)\n",
    "                # Reduce learning rate for SOLIDER stage\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.1\n",
    "            \n",
    "            return self._train_epoch_stage2(dataloader, epoch, total_epochs)\n",
    "    \n",
    "    def _train_epoch_stage1(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"Stage 1: FIDI training only.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0  # Always 0 in stage 1\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Standard forward pass (no semantic loss)\n",
    "            features, logits = self.model(images, return_semantic_loss=False)\n",
    "            \n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            # Optimization step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = 0.0\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'FIDI Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def _train_epoch_stage2(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"Stage 2: SOLIDER training with semantic supervision.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        # Generate lambda values for semantic control\n",
    "        num_batches = len(dataloader)\n",
    "        lambda_vals = torch.bernoulli(torch.full((num_batches,), 0.5))\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Get lambda value for this batch\n",
    "            current_lambda = float(lambda_vals[batch_idx % len(lambda_vals)].item())\n",
    "            \n",
    "            # Forward pass with semantic loss\n",
    "            try:\n",
    "                actual_model = self.get_model()\n",
    "                features, logits, semantic_output = actual_model(\n",
    "                    images, lambda_val=current_lambda, return_semantic_loss=True\n",
    "                )\n",
    "                \n",
    "                # Extract semantic loss safely\n",
    "                if isinstance(semantic_output, dict) and 'semantic_loss' in semantic_output:\n",
    "                    semantic_loss = semantic_output['semantic_loss']\n",
    "                else:\n",
    "                    semantic_loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: SOLIDER forward failed: {e}, using fallback\")\n",
    "                features, logits = self.model(images)\n",
    "                semantic_loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "            \n",
    "            # Compute losses\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Get weights and combine losses\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "            loss = (fidi_weight * fidi_loss + \n",
    "                   cls_weight * ce_loss + \n",
    "                   self.semantic_weight * semantic_loss)\n",
    "            \n",
    "            # Optimization step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track losses safely\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = semantic_loss.item() if hasattr(semantic_loss, 'item') else float(semantic_loss)\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'SOLIDER Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}, '\n",
    "                      f'Semantic={batch_semantic:.6f}×{self.semantic_weight:.2f}, '\n",
    "                      f'Lambda={current_lambda:.1f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        \"\"\"Evaluation with proper SOLIDER model handling.\"\"\"\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        # Optimal lambda for evaluation (appearance-focused)\n",
    "        eval_lambda = 0.15\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                \n",
    "                try:\n",
    "                    actual_model = self.get_model()\n",
    "                    features, logits = actual_model(\n",
    "                        images, lambda_val=eval_lambda, return_semantic_loss=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Lambda-based eval failed: {e}, using standard forward\")\n",
    "                    features, logits = self.model(images)\n",
    "                \n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                \n",
    "                try:\n",
    "                    actual_model = self.get_model()\n",
    "                    features, logits = actual_model(\n",
    "                        images, lambda_val=eval_lambda, return_semantic_loss=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Lambda-based eval failed: {e}, using standard forward\")\n",
    "                    features, logits = self.model(images)\n",
    "                \n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        \"\"\"CMC and mAP computation (unchanged from your original).\"\"\"\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Fine-grained Difference-aware (FIDI) Pairwise Loss\n",
    "    Corrected implementation following the paper exactly\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.05, beta=0.5):\n",
    "        super(FIDILoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: tensor of shape (batch_size, feature_dim)\n",
    "            labels: tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = self.compute_pairwise_distances(features)\n",
    "        \n",
    "        # Compute ground truth binary relationship matrix K\n",
    "        labels = labels.view(-1, 1)\n",
    "        k_matrix = (labels == labels.T).float()  # 1 if same identity, 0 otherwise\n",
    "        \n",
    "        # Compute learned probability distribution U using exponential function\n",
    "        u_matrix = torch.exp(-self.beta * distances)\n",
    "        \n",
    "        # Compute D(U||K) + D(K||U)\n",
    "        d_u_k = self.compute_kl_divergence(u_matrix, k_matrix)\n",
    "        d_k_u = self.compute_kl_divergence(k_matrix, u_matrix)\n",
    "        \n",
    "        total_loss = d_u_k + d_k_u\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_pairwise_distances(self, features):\n",
    "        \"\"\"Compute Euclidean distances between all pairs of features\"\"\"\n",
    "        n = features.size(0)\n",
    "        # Expand features to compute all pairwise distances\n",
    "        features_1 = features.unsqueeze(1).expand(n, n, -1)\n",
    "        features_2 = features.unsqueeze(0).expand(n, n, -1)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distances = torch.sqrt(torch.sum((features_1 - features_2) ** 2, dim=2) + self.eps)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_kl_divergence(self, p_matrix, q_matrix):\n",
    "        \"\"\"\n",
    "        Compute KL divergence D(P||Q) following Equation (5) from the paper:\n",
    "        D(P||Q) = Σ p_ij * log(α * p_ij / ((α-1) * p_ij + q_ij))\n",
    "        \"\"\"\n",
    "        # Clamp to avoid numerical issues\n",
    "        p_matrix = torch.clamp(p_matrix, min=self.eps, max=1-self.eps)\n",
    "        q_matrix = torch.clamp(q_matrix, min=self.eps, max=1-self.eps)\n",
    "        \n",
    "        # Compute the denominator: (α-1) * p_ij + q_ij\n",
    "        denominator = (self.alpha - 1) * p_matrix + q_matrix\n",
    "        denominator = torch.clamp(denominator, min=self.eps)\n",
    "        \n",
    "        # Compute the fraction: α * p_ij / denominator\n",
    "        numerator = self.alpha * p_matrix\n",
    "        fraction = numerator / denominator\n",
    "        fraction = torch.clamp(fraction, min=self.eps)\n",
    "        \n",
    "        # Compute KL divergence: p_ij * log(fraction)\n",
    "        kl_div = p_matrix * torch.log(fraction)\n",
    "        \n",
    "        # Exclude diagonal elements (self-comparisons) and compute mean\n",
    "        mask = ~torch.eye(p_matrix.size(0), dtype=torch.bool, device=p_matrix.device)\n",
    "        kl_div = kl_div[mask].mean()\n",
    "        \n",
    "        return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e36ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Trainer Class\n",
    "# =========================\n",
    "class FIDITrainer:\n",
    "    \"\"\"\n",
    "    Improved Training framework for Person Re-ID with FIDI loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive'):\n",
    "        # Multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # For adaptive strategy\n",
    "        self.loss_history = {'fidi': [], 'ce': []}\n",
    "        self.best_mAP = 0.0\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"\n",
    "        Multiple loss weighting strategies based on training progress and loss magnitudes\n",
    "        \"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # More conservative approach - slower FIDI ramp-up, maintain CE importance\n",
    "            fidi_weight = min(0.8, progress * 1.5)  # Max 0.8, reaches it at 53% of training\n",
    "            cls_weight = max(0.8, 1.2 - progress)   # Min 0.8, gradual decrease\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Gradual transition with smooth curves\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))  # Sigmoid-like curve\n",
    "            cls_weight = 1.0 - 0.3 * progress  # Linear decrease to 0.7\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adaptive based on loss magnitudes (requires loss history)\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                # Calculate recent loss ratios\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                # Balance weights based on loss magnitudes\n",
    "                if recent_fidi > recent_ce * 2:  # FIDI much larger\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:  # CE much larger\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:  # Balanced\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                # Early training fallback\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            # Simple fixed weights\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original' - your current strategy\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        \n",
    "        # Get dynamic weights for this epoch\n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            features, logits = self.model(images)\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Apply dynamic weighting\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store all batch values\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_fidi_loss = total_fidi_loss / len(dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        std_loss = np.std(batch_losses)\n",
    "        \n",
    "        print(f'Epoch Summary: Avg Loss={avg_loss:.6f}, Min={min_loss:.6f}, Max={max_loss:.6f}, Std={std_loss:.6f}')\n",
    "        print(f'FIDI: Avg={avg_fidi_loss:.6f}, Min={min(batch_fidi_losses):.6f}, Max={max(batch_fidi_losses):.6f}')\n",
    "        print(f'CE: Avg={avg_ce_loss:.6f}, Min={min(batch_ce_losses):.6f}, Max={max(batch_ce_losses):.6f}')\n",
    "        \n",
    "        # Store loss history for adaptive strategy\n",
    "        self.loss_history['fidi'].append(avg_fidi_loss)\n",
    "        self.loss_history['ce'].append(avg_ce_loss)\n",
    "        if len(self.loss_history['fidi']) > 20:  # Keep only recent history\n",
    "            self.loss_history['fidi'].pop(0)\n",
    "            self.loss_history['ce'].pop(0)\n",
    "        \n",
    "        return avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n",
    "    \n",
    "    def train(self, train_dataloader, query_dataloader, gallery_dataloader, \n",
    "              num_epochs=120, eval_freq=10):\n",
    "        print(f\"Starting training with '{self.loss_strategy}' loss weighting strategy...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Get current weights for logging\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, num_epochs)\n",
    "            print(f'Loss weights - FIDI: {fidi_weight:.3f}, CE: {cls_weight:.3f}')\n",
    "            \n",
    "            avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses = self.train_epoch(\n",
    "                train_dataloader, epoch, num_epochs\n",
    "            )\n",
    "            print(f'Train Loss: {avg_loss:.4f}, FIDI Loss: {avg_fidi_loss:.4f}, '\n",
    "                  f'CE Loss: {avg_ce_loss:.4f}')\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                cmc, mAP = self.evaluate(query_dataloader, gallery_dataloader)\n",
    "                print(f'Rank-1: {cmc[0]:.4f}, Rank-5: {cmc[4]:.4f}, '\n",
    "                      f'Rank-10: {cmc[9]:.4f}, mAP: {mAP:.4f}')\n",
    "                \n",
    "                if mAP > self.best_mAP:\n",
    "                    self.best_mAP = mAP\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                        'mAP': mAP,\n",
    "                        'cmc': cmc,\n",
    "                        'loss_strategy': self.loss_strategy,\n",
    "                        'fidi_weight': fidi_weight,\n",
    "                        'cls_weight': cls_weight,\n",
    "                    }, 'best_model.pth')\n",
    "                    print(f'New best mAP: {self.best_mAP:.4f}')\n",
    "        \n",
    "        print(f'\\nTraining completed. Best mAP: {self.best_mAP:.4f}')\n",
    "        return self.best_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Tune-able Parameters / Config\n",
    "# =========================\n",
    "# PK Sampling parameters\n",
    "P = 16  # Number of persons per batch\n",
    "K = 4   # Number of images per person\n",
    "batch_size = P * K  # This will be 64 for optimal PK sampling\n",
    "\n",
    "num_epochs = 250\n",
    "device = [0, 1] if torch.cuda.device_count() > 1 else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alpha = 1.05\n",
    "beta = 0.5\n",
    "lr = 3.5e-4\n",
    "weight_decay = 5e-4\n",
    "num_workers = 8\n",
    "prefetch_factor = 4\n",
    "image_height = 256\n",
    "image_width = 128\n",
    "train_dir = os.path.join('Dataset', 'train')\n",
    "query_dir = os.path.join('Dataset', 'query')\n",
    "gallery_dir = os.path.join('Dataset', 'gallery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train samples: 19772, PIDs: 751\n",
      "✓ DataLoaders ready: train 46 batches, query 53, gallery 309\n"
     ]
    }
   ],
   "source": [
    "# 7. Data Transforms & DataLoaders – SOLIDER-only (no fallbacks)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.Pad(10, padding_mode='edge'),\n",
    "    transforms.RandomCrop((image_height, image_width)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.15, 0.15, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.4), ratio=(0.3,3.3), value='random'),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = PersonReIDTrainDataset(train_dir, transform=train_transform)\n",
    "query_dataset = PersonReIDTestDataset(query_dir, transform=test_transform)\n",
    "gallery_dataset = PersonReIDTestDataset(gallery_dir, transform=test_transform)\n",
    "\n",
    "num_classes = len(train_dataset.label_map)\n",
    "\n",
    "# PKSampler (error if not enough PIDs/images)\n",
    "pk_sampler = PKSampler(train_dataset, P=P, K=K)\n",
    "\n",
    "# Always use PK sampling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=pk_sampler,\n",
    "    batch_size=P*K,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,\n",
    "    batch_size=P*K,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "gallery_loader = DataLoader(\n",
    "    gallery_dataset,\n",
    "    batch_size=P*K,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset)}, PIDs: {num_classes}\")\n",
    "print(f\"✓ DataLoaders ready: train {len(train_loader)} batches, \"\n",
    "      f\"query {len(query_loader)}, gallery {len(gallery_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3092619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SOLIDER model with 751 classes\n",
      "✓ Trainer initialized – SOLIDER stage from epoch 0 onwards\n",
      "Using device(s): [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 8. SOLIDER Model & Trainer Initialization – no fallbacks\n",
    "\n",
    "# Model\n",
    "model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "model = model.to(device if not isinstance(device, (list,tuple)) else f\"cuda:{device[0]}\")\n",
    "\n",
    "# Trainer\n",
    "trainer = SOLIDERFIDITrainer(\n",
    "    model=model,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    loss_strategy='progressive',\n",
    "    semantic_weight=0.5\n",
    ")\n",
    "\n",
    "# Enforce SOLIDER stage from epoch 0\n",
    "trainer.stage_switch_epoch = 0\n",
    "\n",
    "print(f\"✓ SOLIDER model with {num_classes} classes\")\n",
    "print(f\"✓ Trainer initialized – SOLIDER stage from epoch 0 onwards\")\n",
    "print(f\"Using device(s): {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df92d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # ONNX Export for SOLIDER Model (Netron Visualization) - SIMPLIFIED CORE\n",
    "# # =========================\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "\n",
    "# def export_solider_model_to_onnx():\n",
    "#     \"\"\"\n",
    "#     Export the SOLIDER CNN model to ONNX format for Netron visualization\n",
    "#     \"\"\"\n",
    "#     print(\"Exporting SOLIDER CNN model to ONNX...\")\n",
    "    \n",
    "#     # Create SOLIDER model instance\n",
    "#     solider_model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "    \n",
    "#     # Determine device for dummy input\n",
    "#     if isinstance(device, (list, tuple)):\n",
    "#         dummy_device = f\"cuda:{device[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     else:\n",
    "#         dummy_device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "#     # Move model to device\n",
    "#     solider_model = solider_model.to(dummy_device)\n",
    "#     solider_model.eval()\n",
    "    \n",
    "#     # Create sample input tensor\n",
    "#     batch_size = 1\n",
    "#     sample_input = torch.randn(batch_size, 3, image_height, image_width, device=dummy_device)\n",
    "    \n",
    "#     # Define ONNX export paths\n",
    "#     onnx_dir = \"onnx_models\"\n",
    "#     os.makedirs(onnx_dir, exist_ok=True)\n",
    "    \n",
    "#     # Create a wrapper that exports just the core backbone without the final layers\n",
    "#     class SOLIDERCoreWrapper(nn.Module):\n",
    "#         def __init__(self, model):\n",
    "#             super().__init__()\n",
    "#             self.model = model\n",
    "            \n",
    "#             # Extract the core backbone (stages 0-4)\n",
    "#             self.stage0 = model.stage0\n",
    "#             self.stage1 = model.stage1\n",
    "#             self.stage2 = model.stage2\n",
    "#             self.stage3 = model.stage3\n",
    "#             self.stage4 = model.stage4\n",
    "            \n",
    "#             # Extract multi-scale fusion\n",
    "#             self.multi_scale_fusion = model.multi_scale_fusion\n",
    "            \n",
    "#             # Extract semantic clustering (without final layers)\n",
    "#             self.semantic_clustering = model.semantic_clustering\n",
    "        \n",
    "#         def forward(self, x):\n",
    "#             # Forward through stages\n",
    "#             stage0_out = self.stage0(x)\n",
    "#             stage1_out = self.stage1(stage0_out)\n",
    "#             stage2_out = self.stage2(stage1_out)\n",
    "#             stage3_out = self.stage3(stage2_out)\n",
    "#             stage4_out = self.stage4(stage3_out)\n",
    "            \n",
    "#             # Multi-scale fusion\n",
    "#             fused_features = self.multi_scale_fusion([stage1_out, stage2_out, stage3_out, stage4_out])\n",
    "            \n",
    "#             # Global pooling\n",
    "#             pooled_features = torch.nn.functional.adaptive_avg_pool2d(fused_features, (1, 1))\n",
    "#             pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "            \n",
    "#             # Return intermediate features for visualization\n",
    "#             return pooled_features, stage4_out\n",
    "    \n",
    "#     # Export core architecture\n",
    "#     core_onnx_path = os.path.join(onnx_dir, \"solider_core_architecture.onnx\")\n",
    "#     core_wrapper = SOLIDERCoreWrapper(solider_model)\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Exporting core SOLIDER architecture...\")\n",
    "#         torch.onnx.export(\n",
    "#             core_wrapper,\n",
    "#             sample_input,\n",
    "#             core_onnx_path,\n",
    "#             export_params=True,\n",
    "#             opset_version=11,\n",
    "#             do_constant_folding=True,\n",
    "#             input_names=['input_image'],\n",
    "#             output_names=['pooled_features', 'stage4_features'],\n",
    "#             dynamic_axes={\n",
    "#                 'input_image': {0: 'batch_size'}, \n",
    "#                 'pooled_features': {0: 'batch_size'}, \n",
    "#                 'stage4_features': {0: 'batch_size'}\n",
    "#             },\n",
    "#             verbose=False\n",
    "#         )\n",
    "#         print(f\"✓ SOLIDER core architecture exported to: {core_onnx_path}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to export core architecture: {str(e)}\")\n",
    "        \n",
    "#         # Try an even simpler approach - just the backbone stages\n",
    "#         print(\"Trying simplified backbone export...\")\n",
    "        \n",
    "#         class SOLIDERBackboneWrapper(nn.Module):\n",
    "#             def __init__(self, model):\n",
    "#                 super().__init__()\n",
    "#                 self.stage0 = model.stage0\n",
    "#                 self.stage1 = model.stage1\n",
    "#                 self.stage2 = model.stage2\n",
    "#                 self.stage3 = model.stage3\n",
    "#                 self.stage4 = model.stage4\n",
    "            \n",
    "#             def forward(self, x):\n",
    "#                 x = self.stage0(x)\n",
    "#                 x = self.stage1(x)\n",
    "#                 x = self.stage2(x)\n",
    "#                 x = self.stage3(x)\n",
    "#                 x = self.stage4(x)\n",
    "#                 return x\n",
    "        \n",
    "#         backbone_onnx_path = os.path.join(onnx_dir, \"solider_backbone_stages.onnx\")\n",
    "#         backbone_wrapper = SOLIDERBackboneWrapper(solider_model)\n",
    "        \n",
    "#         try:\n",
    "#             torch.onnx.export(\n",
    "#                 backbone_wrapper,\n",
    "#                 sample_input,\n",
    "#                 backbone_onnx_path,\n",
    "#                 export_params=True,\n",
    "#                 opset_version=11,\n",
    "#                 do_constant_folding=True,\n",
    "#                 input_names=['input_image'],\n",
    "#                 output_names=['backbone_output'],\n",
    "#                 dynamic_axes={\n",
    "#                     'input_image': {0: 'batch_size'}, \n",
    "#                     'backbone_output': {0: 'batch_size'}\n",
    "#                 },\n",
    "#                 verbose=False\n",
    "#             )\n",
    "#             print(f\"✓ SOLIDER backbone stages exported to: {backbone_onnx_path}\")\n",
    "#             core_onnx_path = backbone_onnx_path\n",
    "            \n",
    "#         except Exception as e2:\n",
    "#             print(f\"❌ Failed to export backbone stages: {str(e2)}\")\n",
    "#             return None\n",
    "    \n",
    "#     # Print model statistics\n",
    "#     total_params = sum(p.numel() for p in solider_model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in solider_model.parameters() if p.requires_grad)\n",
    "    \n",
    "#     print(f\"\\n📊 SOLIDER Model Statistics:\")\n",
    "#     print(f\"   • Total parameters: {total_params:,}\")\n",
    "#     print(f\"   • Trainable parameters: {trainable_params:,}\")\n",
    "#     print(f\"   • Input shape: {tuple(sample_input.shape)}\")\n",
    "#     print(f\"   • Number of classes: {num_classes}\")\n",
    "    \n",
    "#     print(f\"\\n🔍 Netron Visualization:\")\n",
    "#     print(f\"   1. Open Netron (https://netron.app/)\")\n",
    "#     print(f\"   2. Load the exported ONNX file: {core_onnx_path}\")\n",
    "    \n",
    "#     print(f\"\\n💡 Key SOLIDER Components to Look For:\")\n",
    "#     print(f\"   • Multi-scale feature fusion (stages 1-4)\")\n",
    "#     print(f\"   • Spatial semantic clustering\")\n",
    "#     print(f\"   • Semantic controller modules\")\n",
    "#     print(f\"   • ResNet backbone with SOLIDER blocks\")\n",
    "    \n",
    "#     return {'core': core_onnx_path}\n",
    "\n",
    "# # Execute the export\n",
    "# try:\n",
    "#     exported_models = export_solider_model_to_onnx()\n",
    "#     if exported_models:\n",
    "#         print(\"\\n✅ SOLIDER model successfully exported to ONNX!\")\n",
    "#         print(\"   You can now visualize it in Netron!\")\n",
    "#     else:\n",
    "#         print(\"\\n❌ Failed to export SOLIDER model\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error during export: {str(e)}\")\n",
    "#     print(\"Make sure the SOLIDERPersonReIDModel class has been defined and all dependencies are imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d5e6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnsupportedNodeError",
     "evalue": "try blocks aren't supported:\n  File \"/tmp/ipykernel_2064917/1842271771.py\", line 410\n        \n        # Multi-scale fusion\n        try:\n        ~~~ <--- HERE\n            fused_features = self.multi_scale_fusion([x1, x2, x3, x4])\n        except Exception as e:\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedNodeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Save TorchScript model (.pt file) - also in weights folder\u001b[39;00m\n\u001b[1;32m    111\u001b[0m model_to_save \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m--> 112\u001b[0m scripted \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_save\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m script_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights/checkpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m scripted\u001b[38;5;241m.\u001b[39msave(script_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_script.py:1443\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[1;32m   1442\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[1;32m   1452\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_script.py:1152\u001b[0m, in \u001b[0;36m_script_impl\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1151\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     obj \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1157\u001b[0m         obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__()\n\u001b[1;32m   1158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[1;32m   1160\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_recursive.py:556\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    555\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_recursive.py:569\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;124;03mConvert an nn.Module to a RecursiveScriptModule.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_create_module_with_type(concrete_type\u001b[38;5;241m.\u001b[39mjit_type)\n\u001b[0;32m--> 569\u001b[0m method_stubs \u001b[38;5;241m=\u001b[39m \u001b[43mstubs_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m property_stubs \u001b[38;5;241m=\u001b[39m get_property_stubs(nn_module)\n\u001b[1;32m    571\u001b[0m hook_stubs, pre_hook_stubs \u001b[38;5;241m=\u001b[39m get_hook_stubs(nn_module)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_recursive.py:883\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[0;34m(nn_module)\u001b[0m\n\u001b[1;32m    880\u001b[0m     uniqued_methods\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m    881\u001b[0m     uniquer\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[0;32m--> 883\u001b[0m stubs \u001b[38;5;241m=\u001b[39m [\u001b[43mmake_stub_from_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m uniqued_methods]\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overload_stubs \u001b[38;5;241m+\u001b[39m stubs\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_recursive.py:86\u001b[0m, in \u001b[0;36mmake_stub_from_method\u001b[0;34m(nn_module, method_name)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Make sure the name present in the resulting AST will match the name\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# requested here. The only time they don't match is if you do something\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# like:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# In this case, the actual function object will have the name `_forward`,\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# even though we requested a stub for `forward`.\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_stub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/_recursive.py:70\u001b[0m, in \u001b[0;36mmake_stub\u001b[0;34m(func, name)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_stub\u001b[39m(func, name):\n\u001b[1;32m     69\u001b[0m     rcb \u001b[38;5;241m=\u001b[39m _jit_internal\u001b[38;5;241m.\u001b[39mcreateResolutionCallbackFromClosure(func)\n\u001b[0;32m---> 70\u001b[0m     ast \u001b[38;5;241m=\u001b[39m \u001b[43mget_jit_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecursiveScriptModule\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ScriptMethodStub(rcb, ast, func)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/frontend.py:382\u001b[0m, in \u001b[0;36mget_jit_def\u001b[0;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[1;32m    379\u001b[0m     qualname \u001b[38;5;241m=\u001b[39m get_qualified_name(fn)\n\u001b[1;32m    380\u001b[0m     pdt_arg_types \u001b[38;5;241m=\u001b[39m type_trace_db\u001b[38;5;241m.\u001b[39mget_args_types(qualname)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparsed_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_line\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdef_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdt_arg_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdt_arg_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/frontend.py:443\u001b[0m, in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    440\u001b[0m     type_comment_decl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mparse_type_comment(type_line)\n\u001b[1;32m    441\u001b[0m     decl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mmerge_type_from_type_comment(decl, type_comment_decl, is_method)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Def(Ident(r, def_name), decl, \u001b[43mbuild_stmts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/frontend.py:196\u001b[0m, in \u001b[0;36mbuild_stmts\u001b[0;34m(ctx, stmts)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_stmts\u001b[39m(ctx, stmts):\n\u001b[0;32m--> 196\u001b[0m     stmts \u001b[38;5;241m=\u001b[39m [\u001b[43mbuild_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m stmts]\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, stmts))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/jit/frontend.py:415\u001b[0m, in \u001b[0;36mBuilder.__call__\u001b[0;34m(self, ctx, node)\u001b[0m\n\u001b[1;32m    413\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedNodeError(ctx, node)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(ctx, node)\n",
      "\u001b[0;31mUnsupportedNodeError\u001b[0m: try blocks aren't supported:\n  File \"/tmp/ipykernel_2064917/1842271771.py\", line 410\n        \n        # Multi-scale fusion\n        try:\n        ~~~ <--- HERE\n            fused_features = self.multi_scale_fusion([x1, x2, x3, x4])\n        except Exception as e:\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Training Loop with Live Loss & Accuracy Plot\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up logging to file - ONLY ONCE at the beginning\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "log_file = open(log_filename, 'w')\n",
    "\n",
    "# Create a custom print function that writes to both console and file\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    print(*args, **kwargs, file=log_file)\n",
    "    log_file.flush()  # Ensure immediate writing\n",
    "\n",
    "# Log training start\n",
    "log_print(f\"Training started at: {datetime.now()}\")\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(f\"PK sampling: P={P}, K={K}, batch_size={P*K}\")\n",
    "log_print(f\"Number of classes: {num_classes}\")\n",
    "log_print(\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "fidi_losses = []\n",
    "ce_losses = []\n",
    "semantic_losses = []  # ADD THIS FOR SOLIDER\n",
    "epochs = []\n",
    "rank1s = []\n",
    "maps = []\n",
    "eval_epochs = []\n",
    "\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(\"SOLIDER model and trainer initialized successfully!\")\n",
    "log_print(\"Starting training...\")\n",
    "\n",
    "num_epochs_to_run = num_epochs  # You can override this for shorter runs\n",
    "eval_freq = 10  # You can set this in your config cell if you want\n",
    "\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for epoch in range(num_epochs_to_run):\n",
    "    log_print(f'\\nEpoch {epoch+1}/{num_epochs_to_run}')\n",
    "    log_print('-' * 50)\n",
    "    avg_loss, avg_fidi_loss, avg_ce_loss, avg_semantic_loss, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses = trainer.train_epoch(train_loader, epoch, num_epochs_to_run)\n",
    "    train_losses.append(avg_loss)\n",
    "    fidi_losses.append(avg_fidi_loss)\n",
    "    ce_losses.append(avg_ce_loss)\n",
    "    semantic_losses.append(avg_semantic_loss)  # ADD THIS\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # Evaluate and collect accuracy/mAP\n",
    "    if (epoch + 1) % eval_freq == 0 or (epoch + 1) == num_epochs_to_run:\n",
    "        log_print(\"Evaluating...\")\n",
    "        cmc, mAP = trainer.evaluate(query_loader, gallery_loader)\n",
    "        rank1 = float(cmc[0].item())\n",
    "        rank1s.append(rank1)\n",
    "        maps.append(float(mAP))\n",
    "        eval_epochs.append(epoch + 1)\n",
    "        log_print(f'Rank-1: {rank1:.4f}, mAP: {mAP:.4f}')\n",
    "\n",
    "    # Live plot\n",
    "    clear_output(wait=True)\n",
    "    ax1.clear()\n",
    "    ax1.plot(epochs, train_losses, label='Total Loss')\n",
    "    ax1.plot(epochs, fidi_losses, label='FIDI Loss')\n",
    "    ax1.plot(epochs, ce_losses, label='CE Loss')\n",
    "    ax1.plot(epochs, semantic_losses, label='Semantic Loss')  # ADD THIS\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.plot(eval_epochs, rank1s, label='Rank-1 Accuracy')\n",
    "    ax2.plot(eval_epochs, maps, label='mAP')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Validation: Rank-1 & mAP')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    display(fig)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    trainer.scheduler.step()\n",
    "\n",
    "    # Save TorchScript model every 5 epochs\n",
    "    # Save both TorchScript (.pt) and PyTorch (.pth) models every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Create weights directory if it doesn't exist\n",
    "        os.makedirs(\"weights\", exist_ok=True)\n",
    "        \n",
    "        # Save PyTorch state dict (.pth file)\n",
    "        pth_path = f\"weights/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': trainer.model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict()\n",
    "        }, pth_path)\n",
    "        \n",
    "        # Save TorchScript model (.pt file) - also in weights folder\n",
    "        model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "        scripted = torch.jit.script(model_to_save)\n",
    "        script_path = f\"weights/checkpoint_epoch_{epoch+1}.pt\"\n",
    "        scripted.save(script_path)\n",
    "        \n",
    "        log_print(f\"Models saved - PyTorch: {pth_path}, TorchScript: {script_path}\")\n",
    "\n",
    "# Close the log file at the end\n",
    "log_file.close()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "log_print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
