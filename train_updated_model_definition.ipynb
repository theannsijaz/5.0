{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3945b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class PKSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Fixed PK Sampler for Person Re-ID: P persons × K images per person\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, P=16, K=4):\n",
    "        self.data_source = data_source\n",
    "        self.P = P  # Number of persons per batch\n",
    "        self.K = K  # Number of images per person\n",
    "        \n",
    "        # Group samples by person ID\n",
    "        self.pid_to_indices = defaultdict(list)\n",
    "        for idx, (_, pid) in enumerate(data_source.samples):\n",
    "            self.pid_to_indices[pid].append(idx)\n",
    "        \n",
    "        # Filter out persons with less than K images\n",
    "        self.valid_pids = [pid for pid, indices in self.pid_to_indices.items() \n",
    "                          if len(indices) >= self.K]\n",
    "        \n",
    "        if len(self.valid_pids) < self.P:\n",
    "            raise ValueError(f\"Not enough persons with at least {self.K} images. \"\n",
    "                           f\"Found {len(self.valid_pids)}, need {self.P}\")\n",
    "        \n",
    "        # Calculate total number of samples we'll generate\n",
    "        self.num_batches = len(self.valid_pids) // self.P\n",
    "        self.total_size = self.num_batches * self.P * self.K\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Fixed iterator that yields individual indices, not batches\"\"\"\n",
    "        # Shuffle valid PIDs for each epoch\n",
    "        shuffled_pids = self.valid_pids.copy()\n",
    "        random.shuffle(shuffled_pids)\n",
    "        \n",
    "        # Generate all indices for this epoch\n",
    "        all_indices = []\n",
    "        \n",
    "        for batch_start in range(0, len(shuffled_pids) - self.P + 1, self.P):\n",
    "            # Select P persons for this batch\n",
    "            batch_pids = shuffled_pids[batch_start:batch_start + self.P]\n",
    "            \n",
    "            batch_indices = []\n",
    "            for pid in batch_pids:\n",
    "                # Randomly select K images for this person\n",
    "                available_indices = self.pid_to_indices[pid]\n",
    "                if len(available_indices) >= self.K:\n",
    "                    selected_indices = random.sample(available_indices, self.K)\n",
    "                    batch_indices.extend(selected_indices)\n",
    "            \n",
    "            # Shuffle within batch to avoid ordering bias\n",
    "            random.shuffle(batch_indices)\n",
    "            all_indices.extend(batch_indices)\n",
    "        \n",
    "        # Yield individual indices\n",
    "        for idx in all_indices:\n",
    "            yield idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "class PersonReIDTrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training set: expects structure Dataset/train/<pid>/*.jpg\n",
    "    Returns (image, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label)\n",
    "        self.label_map = {}  # pid (str) -> label (int)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        pids = sorted(os.listdir(self.root_dir))\n",
    "        self.label_map = {pid: idx for idx, pid in enumerate(pids)}\n",
    "        for pid in pids:\n",
    "            pid_dir = os.path.join(self.root_dir, pid)\n",
    "            if not os.path.isdir(pid_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(pid_dir):\n",
    "                if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.samples.append((os.path.join(pid_dir, fname), self.label_map[pid]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load image {img_path}: {e}\")\n",
    "            # Return a dummy image\n",
    "            dummy_img = torch.zeros(3, 256, 128)\n",
    "            return dummy_img, label\n",
    "\n",
    "class PersonReIDTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for query/gallery set: expects structure Dataset/query/*.jpg or Dataset/gallery/*.jpg\n",
    "    Returns (image, label, cam_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label, cam_id)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        for fname in os.listdir(self.dir_path):\n",
    "            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                try:\n",
    "                    # Example: 0001_c1s1_001051_00.jpg\n",
    "                    parts = fname.split('_')\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                    label = int(parts[0])\n",
    "                    cam_id = int(parts[1][1])  # e.g., c1 -> 1\n",
    "                    self.samples.append((os.path.join(self.dir_path, fname), label, cam_id))\n",
    "                except (ValueError, IndexError) as e:\n",
    "                    print(f\"Warning: Skipping file {fname} due to naming format: {e}\")\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, cam_id = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, label, cam_id\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load image {img_path}: {e}\")\n",
    "            # Return a dummy image\n",
    "            dummy_img = torch.zeros(3, 256, 128)\n",
    "            return dummy_img, label, cam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ff540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatialSemanticClustering(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved spatial-level semantic clustering for CNN feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, num_semantic_parts=3, momentum=0.99):\n",
    "        super(SpatialSemanticClustering, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_semantic_parts = num_semantic_parts\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Semantic head with dropout for robustness\n",
    "        self.semantic_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 2),\n",
    "            nn.BatchNorm1d(feature_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(feature_dim // 2, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(feature_dim // 4, num_semantic_parts + 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def spatial_semantic_labeling(self, feature_maps):\n",
    "        \"\"\"Generate spatial semantic labels based on human priors.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        # Create spatial coordinate grids\n",
    "        y_coords = torch.linspace(0, 1, H, device=device).view(H, 1).expand(H, W)\n",
    "        \n",
    "        # Human prior: spatial semantic assignment\n",
    "        semantic_labels = torch.zeros(H, W, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Upper body: top 40%\n",
    "        upper_mask = y_coords < 0.4\n",
    "        semantic_labels[upper_mask] = 0\n",
    "        \n",
    "        # Lower body: middle 40%\n",
    "        middle_mask = (y_coords >= 0.4) & (y_coords < 0.8)\n",
    "        semantic_labels[middle_mask] = 1\n",
    "        \n",
    "        # Shoes: bottom 20%\n",
    "        lower_mask = y_coords >= 0.8\n",
    "        semantic_labels[lower_mask] = 2\n",
    "        \n",
    "        return semantic_labels\n",
    "    \n",
    "    def foreground_background_clustering(self, feature_maps):\n",
    "        \"\"\"Separate foreground and background with improved stability.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        \n",
    "        # Calculate feature magnitude\n",
    "        feature_magnitude = torch.norm(feature_maps, dim=1, p=2)  # [B, H, W]\n",
    "        \n",
    "        # Adaptive threshold with clamping for stability\n",
    "        batch_mean = feature_magnitude.mean(dim=(1, 2), keepdim=True)\n",
    "        batch_std = feature_magnitude.std(dim=(1, 2), keepdim=True)\n",
    "        \n",
    "        # Clamp std to avoid division by zero\n",
    "        batch_std = torch.clamp(batch_std, min=1e-6)\n",
    "        fg_threshold = batch_mean + 0.5 * batch_std\n",
    "        \n",
    "        # Create foreground mask\n",
    "        fg_mask = feature_magnitude > fg_threshold\n",
    "        \n",
    "        return fg_mask\n",
    "    \n",
    "    def forward(self, student_features, teacher_features=None):\n",
    "        \"\"\"Forward pass with improved error handling.\"\"\"\n",
    "        B, C, H, W = student_features.shape\n",
    "        device = student_features.device\n",
    "        \n",
    "        try:\n",
    "            # Use teacher features if available\n",
    "            clustering_features = teacher_features if teacher_features is not None else student_features\n",
    "            \n",
    "            # Generate pseudo semantic labels\n",
    "            fg_mask = self.foreground_background_clustering(clustering_features)\n",
    "            spatial_labels = self.spatial_semantic_labeling(clustering_features)\n",
    "            \n",
    "            # Combine foreground mask with spatial labels\n",
    "            pseudo_labels = torch.full((B, H, W), self.num_semantic_parts, \n",
    "                                     dtype=torch.long, device=device)\n",
    "            \n",
    "            for b in range(B):\n",
    "                fg_positions = fg_mask[b]\n",
    "                if torch.any(fg_positions):  # Check if any foreground pixels exist\n",
    "                    pseudo_labels[b][fg_positions] = spatial_labels[fg_positions]\n",
    "            \n",
    "            # Flatten for classification\n",
    "            student_flat = student_features.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "            labels_flat = pseudo_labels.reshape(-1)\n",
    "            \n",
    "            # Semantic classification with better error handling\n",
    "            if student_flat.size(0) > 0:\n",
    "                semantic_logits = self.semantic_head(student_flat)\n",
    "                # Use label smoothing for better training stability\n",
    "                semantic_loss = F.cross_entropy(semantic_logits, labels_flat, \n",
    "                                               reduction='mean', label_smoothing=0.1)\n",
    "            else:\n",
    "                semantic_logits = torch.zeros(B*H*W, self.num_semantic_parts + 1, device=device)\n",
    "                semantic_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "            return {\n",
    "                'semantic_loss': semantic_loss,\n",
    "                'pseudo_labels': pseudo_labels,\n",
    "                'foreground_mask': fg_mask,\n",
    "                'semantic_logits': semantic_logits.view(B, H, W, -1)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Semantic clustering failed: {e}\")\n",
    "            # Safe fallback\n",
    "            return {\n",
    "                'semantic_loss': torch.tensor(0.0, device=device, requires_grad=True),\n",
    "                'pseudo_labels': torch.zeros(B, H, W, dtype=torch.long, device=device),\n",
    "                'foreground_mask': torch.ones(B, H, W, dtype=torch.bool, device=device),\n",
    "                'semantic_logits': torch.zeros(B, H, W, self.num_semantic_parts + 1, device=device)\n",
    "            }\n",
    "\n",
    "class SemanticController(nn.Module):\n",
    "    \"\"\"Improved semantic controller with better parameter handling.\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(SemanticController, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Lambda encoding networks\n",
    "        self.weight_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.bias_encoder = nn.Sequential(\n",
    "            nn.Linear(1, feature_dim // 4),\n",
    "            nn.BatchNorm1d(feature_dim // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim // 4, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, feature_maps, lambda_val=0.5):\n",
    "        \"\"\"Apply semantic control with robust parameter handling.\"\"\"\n",
    "        B, C, H, W = feature_maps.shape\n",
    "        device = feature_maps.device\n",
    "        \n",
    "        try:\n",
    "            # Robust lambda value handling\n",
    "            if isinstance(lambda_val, (int, float)):\n",
    "                lambda_tensor = torch.tensor([[float(lambda_val)]], device=device, dtype=torch.float32)\n",
    "            elif isinstance(lambda_val, torch.Tensor):\n",
    "                if lambda_val.dim() == 0:\n",
    "                    lambda_tensor = lambda_val.unsqueeze(0).unsqueeze(0).to(device).float()\n",
    "                elif lambda_val.dim() == 1:\n",
    "                    lambda_tensor = lambda_val.unsqueeze(1).to(device).float()\n",
    "                else:\n",
    "                    lambda_tensor = lambda_val.to(device).float()\n",
    "            else:\n",
    "                lambda_tensor = torch.tensor([[0.5]], device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Ensure we have the right shape [1, 1]\n",
    "            if lambda_tensor.numel() == 0:\n",
    "                lambda_tensor = torch.tensor([[0.5]], device=device, dtype=torch.float32)\n",
    "            elif lambda_tensor.shape != (1, 1):\n",
    "                lambda_tensor = lambda_tensor.view(1, 1)\n",
    "            \n",
    "            # Encode lambda into weights and biases\n",
    "            weights = self.weight_encoder(lambda_tensor)  # [1, C]\n",
    "            biases = self.bias_encoder(lambda_tensor)     # [1, C]\n",
    "            \n",
    "            # Expand for broadcasting\n",
    "            weights = weights.view(1, C, 1, 1).expand(B, C, H, W)\n",
    "            biases = biases.view(1, C, 1, 1).expand(B, C, H, W)\n",
    "            \n",
    "            # Apply semantic control\n",
    "            controlled_features = weights * feature_maps + biases\n",
    "            \n",
    "            return controlled_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Semantic control failed: {e}, returning original features\")\n",
    "            return feature_maps\n",
    "\n",
    "class SOLIDERCNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet block integrated with SOLIDER semantic control.\n",
    "    Maintains ResNet structure while adding semantic controllability.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(SOLIDERCNNBlock, self).__init__()\n",
    "        \n",
    "        # Standard ResNet block components\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # SOLIDER semantic controller\n",
    "        self.semantic_controller = SemanticController(out_channels)\n",
    "        \n",
    "    def forward(self, x, lambda_val=0.5):\n",
    "        identity = x\n",
    "        \n",
    "        # Standard ResNet forward pass\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply downsampling if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Apply semantic control before adding residual\n",
    "        out = self.semantic_controller(out, lambda_val)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class MultiScaleFeatureFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed multi-scale feature fusion with proper dimension handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dims=None, output_dim=2048):\n",
    "        super(MultiScaleFeatureFusion, self).__init__()\n",
    "        \n",
    "        # Default ResNet50 dimensions if not provided\n",
    "        if feature_dims is None:\n",
    "            feature_dims = [256, 512, 1024, 2048]\n",
    "        \n",
    "        self.feature_dims = feature_dims\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Projection layers to align dimensions\n",
    "        self.projections = nn.ModuleList()\n",
    "        for dim in feature_dims:\n",
    "            if dim != output_dim:\n",
    "                self.projections.append(nn.Sequential(\n",
    "                    nn.Conv2d(dim, output_dim, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(output_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                ))\n",
    "            else:\n",
    "                # Identity projection for same dimension\n",
    "                self.projections.append(nn.Identity())\n",
    "        \n",
    "        # Attention mechanism for scale weighting\n",
    "        self.scale_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(output_dim, output_dim // 4, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_dim // 4, len(feature_dims), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, multi_scale_features):\n",
    "        \"\"\"Fuse multi-scale features with proper error handling.\"\"\"\n",
    "        if len(multi_scale_features) != len(self.feature_dims):\n",
    "            print(f\"Warning: Expected {len(self.feature_dims)} features, got {len(multi_scale_features)}\")\n",
    "            # Use only the available features\n",
    "            multi_scale_features = multi_scale_features[:len(self.feature_dims)]\n",
    "            if len(multi_scale_features) == 0:\n",
    "                # Fallback: return zeros\n",
    "                B = 1\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                return torch.zeros(B, self.output_dim, 8, 4, device=device)\n",
    "        \n",
    "        # Get target size from the highest resolution feature (last one)\n",
    "        target_size = multi_scale_features[-1].shape[2:]\n",
    "        \n",
    "        # Project and resize features\n",
    "        projected_features = []\n",
    "        for i, (feat, proj) in enumerate(zip(multi_scale_features, self.projections)):\n",
    "            try:\n",
    "                # Apply projection\n",
    "                projected = proj(feat)\n",
    "                \n",
    "                # Resize if needed\n",
    "                if projected.shape[2:] != target_size:\n",
    "                    projected = F.interpolate(\n",
    "                        projected, size=target_size, \n",
    "                        mode='bilinear', align_corners=False\n",
    "                    )\n",
    "                \n",
    "                projected_features.append(projected)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to process feature {i}: {e}\")\n",
    "                # Skip this feature\n",
    "                continue\n",
    "        \n",
    "        if not projected_features:\n",
    "            # Fallback if all projections failed\n",
    "            B, _, H, W = multi_scale_features[-1].shape\n",
    "            device = multi_scale_features[-1].device\n",
    "            return torch.zeros(B, self.output_dim, H, W, device=device)\n",
    "        \n",
    "        # Stack features for attention computation\n",
    "        stacked_features = torch.stack(projected_features, dim=1)  # [B, num_scales, C, H, W]\n",
    "        B, num_scales, C, H, W = stacked_features.shape\n",
    "        \n",
    "        # Compute attention weights using mean feature\n",
    "        mean_feature = torch.mean(stacked_features, dim=1)  # [B, C, H, W]\n",
    "        attention_weights = self.scale_attention(mean_feature)  # [B, num_scales, 1, 1]\n",
    "        \n",
    "        # Apply attention and fuse\n",
    "        attention_weights = attention_weights.unsqueeze(2)  # [B, num_scales, 1, 1, 1]\n",
    "        weighted_features = stacked_features * attention_weights\n",
    "        fused_features = torch.sum(weighted_features, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "class SOLIDERPersonReIDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Fixed SOLIDER Person Re-ID model with improved stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feature_dim=2048, num_semantic_parts=3):\n",
    "        super(SOLIDERPersonReIDModel, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 backbone\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Extract stages\n",
    "        self.stage0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.stage1 = resnet.layer1  # 256 channels\n",
    "        self.stage2 = resnet.layer2  # 512 channels  \n",
    "        self.stage3 = resnet.layer3  # 1024 channels\n",
    "        self.stage4 = resnet.layer4  # 2048 channels\n",
    "        \n",
    "        # Multi-scale fusion with correct dimensions\n",
    "        self.multi_scale_fusion = MultiScaleFeatureFusion(\n",
    "            feature_dims=[256, 512, 1024, 2048],\n",
    "            output_dim=feature_dim\n",
    "        )\n",
    "        \n",
    "        # Semantic clustering\n",
    "        self.semantic_clustering = SpatialSemanticClustering(\n",
    "            feature_dim=feature_dim,\n",
    "            num_semantic_parts=num_semantic_parts\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn_neck = nn.BatchNorm1d(feature_dim)\n",
    "        self.bn_neck.bias.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes, bias=False)\n",
    "        \n",
    "        self._init_params()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        nn.init.kaiming_normal_(self.classifier.weight, mode='fan_out')\n",
    "        nn.init.constant_(self.bn_neck.weight, 1)\n",
    "        nn.init.constant_(self.bn_neck.bias, 0)\n",
    "    \n",
    "    def forward(self, x, lambda_val=0.5, return_semantic_loss=False, teacher_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass with consistent output handling.\n",
    "        \"\"\"\n",
    "        # Extract multi-scale features\n",
    "        x0 = self.stage0(x)\n",
    "        x1 = self.stage1(x0)\n",
    "        x2 = self.stage2(x1)  \n",
    "        x3 = self.stage3(x2)\n",
    "        x4 = self.stage4(x3)\n",
    "        \n",
    "        # Multi-scale fusion\n",
    "        try:\n",
    "            fused_features = self.multi_scale_fusion([x1, x2, x3, x4])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Multi-scale fusion failed: {e}, using stage4 only\")\n",
    "            fused_features = x4\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        pooled_features = self.global_pool(fused_features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        features = self.bn_neck(pooled_features)\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        # Return based on request\n",
    "        if return_semantic_loss:\n",
    "            try:\n",
    "                semantic_output = self.semantic_clustering(fused_features, teacher_features)\n",
    "                return features, logits, semantic_output\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Semantic clustering failed: {e}\")\n",
    "                # Return with zero semantic loss\n",
    "                device = features.device\n",
    "                semantic_output = {\n",
    "                    'semantic_loss': torch.tensor(0.0, device=device, requires_grad=True)\n",
    "                }\n",
    "                return features, logits, semantic_output\n",
    "        else:\n",
    "            return features, logits\n",
    "\n",
    "def create_solider_model(num_classes):\n",
    "    \"\"\"Factory function to create SOLIDER model.\"\"\"\n",
    "    return SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "\n",
    "\n",
    "class SOLIDERFIDITrainer:\n",
    "    \"\"\"\n",
    "    Fixed SOLIDER-enhanced FIDI trainer with better error handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive', semantic_weight=0.5):\n",
    "        \n",
    "        # Device setup with multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "            self.is_parallel = True\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "            self.is_parallel = False\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        self.semantic_weight = semantic_weight\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # Training state\n",
    "        self.loss_history = {'fidi': [], 'ce': [], 'semantic': []}\n",
    "        self.best_mAP = 0.0\n",
    "        self.stage_switch_epoch = 100  # Switch to SOLIDER stage after this epoch\n",
    "    \n",
    "    def get_model(self):\n",
    "        \"\"\"Get the actual model (handle DataParallel wrapper)\"\"\"\n",
    "        return self.model.module if self.is_parallel else self.model\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"Get dynamic loss weights based on training progress.\"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            fidi_weight = min(0.8, progress * 1.5)\n",
    "            cls_weight = max(0.8, 1.2 - progress)\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))\n",
    "            cls_weight = 1.0 - 0.3 * progress\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                if recent_fidi > recent_ce * 2:\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original'\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        \"\"\"\n",
    "        Fixed train_epoch method that handles both FIDI and SOLIDER stages.\n",
    "        \"\"\"\n",
    "        # Determine training stage\n",
    "        if epoch < self.stage_switch_epoch:\n",
    "            return self._train_epoch_stage1(dataloader, epoch, total_epochs)\n",
    "        else:\n",
    "            if epoch == self.stage_switch_epoch:\n",
    "                print(\"=\" * 50)\n",
    "                print(\"SWITCHING TO SOLIDER STAGE\")\n",
    "                print(\"=\" * 50)\n",
    "                # Reduce learning rate for SOLIDER stage\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.1\n",
    "            \n",
    "            return self._train_epoch_stage2(dataloader, epoch, total_epochs)\n",
    "    \n",
    "    def _train_epoch_stage1(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"Stage 1: FIDI training only.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0  # Always 0 in stage 1\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Standard forward pass (no semantic loss)\n",
    "            features, logits = self.model(images, return_semantic_loss=False)\n",
    "            \n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            # Optimization step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = 0.0\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'FIDI Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def _train_epoch_stage2(self, dataloader, epoch, total_epochs):\n",
    "        \"\"\"Stage 2: SOLIDER training with semantic supervision.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        total_semantic_loss = 0.0\n",
    "        \n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        batch_semantic_losses = []\n",
    "        \n",
    "        # Generate lambda values for semantic control\n",
    "        num_batches = len(dataloader)\n",
    "        lambda_vals = torch.bernoulli(torch.full((num_batches,), 0.5))\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Get lambda value for this batch\n",
    "            current_lambda = float(lambda_vals[batch_idx % len(lambda_vals)].item())\n",
    "            \n",
    "            # Forward pass with semantic loss\n",
    "            try:\n",
    "                actual_model = self.get_model()\n",
    "                features, logits, semantic_output = actual_model(\n",
    "                    images, lambda_val=current_lambda, return_semantic_loss=True\n",
    "                )\n",
    "                \n",
    "                # Extract semantic loss safely\n",
    "                if isinstance(semantic_output, dict) and 'semantic_loss' in semantic_output:\n",
    "                    semantic_loss = semantic_output['semantic_loss']\n",
    "                else:\n",
    "                    semantic_loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: SOLIDER forward failed: {e}, using fallback\")\n",
    "                features, logits = self.model(images)\n",
    "                semantic_loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "            \n",
    "            # Compute losses\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Get weights and combine losses\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "            loss = (fidi_weight * fidi_loss + \n",
    "                   cls_weight * ce_loss + \n",
    "                   self.semantic_weight * semantic_loss)\n",
    "            \n",
    "            # Optimization step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Track losses safely\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            batch_semantic = semantic_loss.item() if hasattr(semantic_loss, 'item') else float(semantic_loss)\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            total_semantic_loss += batch_semantic\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            batch_semantic_losses.append(batch_semantic)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'SOLIDER Stage - Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}, '\n",
    "                      f'Semantic={batch_semantic:.6f}×{self.semantic_weight:.2f}, '\n",
    "                      f'Lambda={current_lambda:.1f}')\n",
    "        \n",
    "        # Calculate averages\n",
    "        num_batches = len(dataloader)\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_fidi = total_fidi_loss / num_batches\n",
    "        avg_ce = total_ce_loss / num_batches\n",
    "        avg_semantic = total_semantic_loss / num_batches\n",
    "        \n",
    "        # Update history\n",
    "        self.loss_history['fidi'].append(avg_fidi)\n",
    "        self.loss_history['ce'].append(avg_ce)\n",
    "        self.loss_history['semantic'].append(avg_semantic)\n",
    "        \n",
    "        return avg_loss, avg_fidi, avg_ce, avg_semantic, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        \"\"\"Evaluation with proper SOLIDER model handling.\"\"\"\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        # Optimal lambda for evaluation (appearance-focused)\n",
    "        eval_lambda = 0.15\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                \n",
    "                try:\n",
    "                    actual_model = self.get_model()\n",
    "                    features, logits = actual_model(\n",
    "                        images, lambda_val=eval_lambda, return_semantic_loss=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Lambda-based eval failed: {e}, using standard forward\")\n",
    "                    features, logits = self.model(images)\n",
    "                \n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                \n",
    "                try:\n",
    "                    actual_model = self.get_model()\n",
    "                    features, logits = actual_model(\n",
    "                        images, lambda_val=eval_lambda, return_semantic_loss=False\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Lambda-based eval failed: {e}, using standard forward\")\n",
    "                    features, logits = self.model(images)\n",
    "                \n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        \"\"\"CMC and mAP computation (unchanged from your original).\"\"\"\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d5d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Fine-grained Difference-aware (FIDI) Pairwise Loss\n",
    "    Corrected implementation following the paper exactly\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.05, beta=0.5):\n",
    "        super(FIDILoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: tensor of shape (batch_size, feature_dim)\n",
    "            labels: tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = self.compute_pairwise_distances(features)\n",
    "        \n",
    "        # Compute ground truth binary relationship matrix K\n",
    "        labels = labels.view(-1, 1)\n",
    "        k_matrix = (labels == labels.T).float()  # 1 if same identity, 0 otherwise\n",
    "        \n",
    "        # Compute learned probability distribution U using exponential function\n",
    "        u_matrix = torch.exp(-self.beta * distances)\n",
    "        \n",
    "        # Compute D(U||K) + D(K||U)\n",
    "        d_u_k = self.compute_kl_divergence(u_matrix, k_matrix)\n",
    "        d_k_u = self.compute_kl_divergence(k_matrix, u_matrix)\n",
    "        \n",
    "        total_loss = d_u_k + d_k_u\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_pairwise_distances(self, features):\n",
    "        \"\"\"Compute Euclidean distances between all pairs of features\"\"\"\n",
    "        n = features.size(0)\n",
    "        # Expand features to compute all pairwise distances\n",
    "        features_1 = features.unsqueeze(1).expand(n, n, -1)\n",
    "        features_2 = features.unsqueeze(0).expand(n, n, -1)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distances = torch.sqrt(torch.sum((features_1 - features_2) ** 2, dim=2) + self.eps)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_kl_divergence(self, p_matrix, q_matrix):\n",
    "        \"\"\"\n",
    "        Compute KL divergence D(P||Q) following Equation (5) from the paper:\n",
    "        D(P||Q) = Σ p_ij * log(α * p_ij / ((α-1) * p_ij + q_ij))\n",
    "        \"\"\"\n",
    "        # Clamp to avoid numerical issues\n",
    "        p_matrix = torch.clamp(p_matrix, min=self.eps, max=1-self.eps)\n",
    "        q_matrix = torch.clamp(q_matrix, min=self.eps, max=1-self.eps)\n",
    "        \n",
    "        # Compute the denominator: (α-1) * p_ij + q_ij\n",
    "        denominator = (self.alpha - 1) * p_matrix + q_matrix\n",
    "        denominator = torch.clamp(denominator, min=self.eps)\n",
    "        \n",
    "        # Compute the fraction: α * p_ij / denominator\n",
    "        numerator = self.alpha * p_matrix\n",
    "        fraction = numerator / denominator\n",
    "        fraction = torch.clamp(fraction, min=self.eps)\n",
    "        \n",
    "        # Compute KL divergence: p_ij * log(fraction)\n",
    "        kl_div = p_matrix * torch.log(fraction)\n",
    "        \n",
    "        # Exclude diagonal elements (self-comparisons) and compute mean\n",
    "        mask = ~torch.eye(p_matrix.size(0), dtype=torch.bool, device=p_matrix.device)\n",
    "        kl_div = kl_div[mask].mean()\n",
    "        \n",
    "        return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e36ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Trainer Class\n",
    "# =========================\n",
    "class FIDITrainer:\n",
    "    \"\"\"\n",
    "    Improved Training framework for Person Re-ID with FIDI loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive'):\n",
    "        # Multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # For adaptive strategy\n",
    "        self.loss_history = {'fidi': [], 'ce': []}\n",
    "        self.best_mAP = 0.0\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"\n",
    "        Multiple loss weighting strategies based on training progress and loss magnitudes\n",
    "        \"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # More conservative approach - slower FIDI ramp-up, maintain CE importance\n",
    "            fidi_weight = min(0.8, progress * 1.5)  # Max 0.8, reaches it at 53% of training\n",
    "            cls_weight = max(0.8, 1.2 - progress)   # Min 0.8, gradual decrease\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Gradual transition with smooth curves\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))  # Sigmoid-like curve\n",
    "            cls_weight = 1.0 - 0.3 * progress  # Linear decrease to 0.7\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adaptive based on loss magnitudes (requires loss history)\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                # Calculate recent loss ratios\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                # Balance weights based on loss magnitudes\n",
    "                if recent_fidi > recent_ce * 2:  # FIDI much larger\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:  # CE much larger\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:  # Balanced\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                # Early training fallback\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            # Simple fixed weights\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original' - your current strategy\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        \n",
    "        # Get dynamic weights for this epoch\n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            features, logits = self.model(images)\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Apply dynamic weighting\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store all batch values\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_fidi_loss = total_fidi_loss / len(dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        std_loss = np.std(batch_losses)\n",
    "        \n",
    "        print(f'Epoch Summary: Avg Loss={avg_loss:.6f}, Min={min_loss:.6f}, Max={max_loss:.6f}, Std={std_loss:.6f}')\n",
    "        print(f'FIDI: Avg={avg_fidi_loss:.6f}, Min={min(batch_fidi_losses):.6f}, Max={max(batch_fidi_losses):.6f}')\n",
    "        print(f'CE: Avg={avg_ce_loss:.6f}, Min={min(batch_ce_losses):.6f}, Max={max(batch_ce_losses):.6f}')\n",
    "        \n",
    "        # Store loss history for adaptive strategy\n",
    "        self.loss_history['fidi'].append(avg_fidi_loss)\n",
    "        self.loss_history['ce'].append(avg_ce_loss)\n",
    "        if len(self.loss_history['fidi']) > 20:  # Keep only recent history\n",
    "            self.loss_history['fidi'].pop(0)\n",
    "            self.loss_history['ce'].pop(0)\n",
    "        \n",
    "        return avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n",
    "    \n",
    "    def train(self, train_dataloader, query_dataloader, gallery_dataloader, \n",
    "              num_epochs=120, eval_freq=10):\n",
    "        print(f\"Starting training with '{self.loss_strategy}' loss weighting strategy...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Get current weights for logging\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, num_epochs)\n",
    "            print(f'Loss weights - FIDI: {fidi_weight:.3f}, CE: {cls_weight:.3f}')\n",
    "            \n",
    "            avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses = self.train_epoch(\n",
    "                train_dataloader, epoch, num_epochs\n",
    "            )\n",
    "            print(f'Train Loss: {avg_loss:.4f}, FIDI Loss: {avg_fidi_loss:.4f}, '\n",
    "                  f'CE Loss: {avg_ce_loss:.4f}')\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                cmc, mAP = self.evaluate(query_dataloader, gallery_dataloader)\n",
    "                print(f'Rank-1: {cmc[0]:.4f}, Rank-5: {cmc[4]:.4f}, '\n",
    "                      f'Rank-10: {cmc[9]:.4f}, mAP: {mAP:.4f}')\n",
    "                \n",
    "                if mAP > self.best_mAP:\n",
    "                    self.best_mAP = mAP\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                        'mAP': mAP,\n",
    "                        'cmc': cmc,\n",
    "                        'loss_strategy': self.loss_strategy,\n",
    "                        'fidi_weight': fidi_weight,\n",
    "                        'cls_weight': cls_weight,\n",
    "                    }, 'best_model.pth')\n",
    "                    print(f'New best mAP: {self.best_mAP:.4f}')\n",
    "        \n",
    "        print(f'\\nTraining completed. Best mAP: {self.best_mAP:.4f}')\n",
    "        return self.best_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e9c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Tune-able Parameters / Config\n",
    "# =========================\n",
    "# PK Sampling parameters\n",
    "P = 16  # Number of persons per batch\n",
    "K = 4   # Number of images per person\n",
    "batch_size = P * K  # This will be 64 for optimal PK sampling\n",
    "\n",
    "num_epochs = 250\n",
    "device = [0, 1] if torch.cuda.device_count() > 1 else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alpha = 1.05\n",
    "beta = 0.5\n",
    "lr = 3.5e-4\n",
    "weight_decay = 5e-4\n",
    "num_workers = 8\n",
    "prefetch_factor = 4\n",
    "image_height = 256\n",
    "image_width = 128\n",
    "train_dir = os.path.join('Dataset', 'train')\n",
    "query_dir = os.path.join('Dataset', 'query')\n",
    "gallery_dir = os.path.join('Dataset', 'gallery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd83123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with PK sampling: P=16, K=4, batch=64\n"
     ]
    }
   ],
   "source": [
    "# 7. Data Transforms & DataLoaders – SOLIDER-only (no fallbacks)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.Pad(10, padding_mode='edge'),\n",
    "    transforms.RandomCrop((image_height, image_width)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.15, 0.15, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.4), ratio=(0.3,3.3), value='random'),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = PersonReIDTrainDataset(train_dir, transform=train_transform)\n",
    "query_dataset = PersonReIDTestDataset(query_dir, transform=test_transform)\n",
    "gallery_dataset = PersonReIDTestDataset(gallery_dir, transform=test_transform)\n",
    "\n",
    "num_classes = len(train_dataset.label_map)\n",
    "\n",
    "# PKSampler (error if not enough PIDs/images)\n",
    "pk_sampler = PKSampler(train_dataset, P=P, K=K)\n",
    "\n",
    "# Always use PK sampling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=pk_sampler,\n",
    "    batch_size=P*K,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,\n",
    "    batch_size=P*K,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "gallery_loader = DataLoader(\n",
    "    gallery_dataset,\n",
    "    batch_size=P*K,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_dataset)}, PIDs: {num_classes}\")\n",
    "print(f\"✓ DataLoaders ready: train {len(train_loader)} batches, \"\n",
    "      f\"query {len(query_loader)}, gallery {len(gallery_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3092619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SOLIDER model created successfully with 751 classes\n",
      "  • Total parameters: 32,411,720\n",
      "  • Trainable parameters: 32,409,672\n",
      "✓ SOLIDER trainer created successfully\n",
      "Using device: cpu\n",
      "Ready to start training with SOLIDERFIDITrainer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. SOLIDER Model & Trainer Initialization – no fallbacks\n",
    "\n",
    "# Model\n",
    "model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "model = model.to(device if not isinstance(device, (list,tuple)) else f\"cuda:{device[0]}\")\n",
    "\n",
    "# Trainer\n",
    "trainer = SOLIDERFIDITrainer(\n",
    "    model=model,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    loss_strategy='progressive',\n",
    "    semantic_weight=0.5\n",
    ")\n",
    "\n",
    "# Enforce SOLIDER stage from epoch 0\n",
    "trainer.stage_switch_epoch = 0\n",
    "\n",
    "print(f\"✓ SOLIDER model with {num_classes} classes\")\n",
    "print(f\"✓ Trainer initialized – SOLIDER stage from epoch 0 onwards\")\n",
    "print(f\"Using device(s): {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting SOLIDER CNN model to ONNX...\n",
      "Exporting core SOLIDER architecture...\n",
      "✓ SOLIDER core architecture exported to: onnx_models/solider_core_architecture.onnx\n",
      "\n",
      "📊 SOLIDER Model Statistics:\n",
      "   • Total parameters: 32,411,720\n",
      "   • Trainable parameters: 32,409,672\n",
      "   • Input shape: (1, 3, 256, 128)\n",
      "   • Number of classes: 751\n",
      "\n",
      "🔍 Netron Visualization:\n",
      "   1. Open Netron (https://netron.app/)\n",
      "   2. Load the exported ONNX file: onnx_models/solider_core_architecture.onnx\n",
      "\n",
      "💡 Key SOLIDER Components to Look For:\n",
      "   • Multi-scale feature fusion (stages 1-4)\n",
      "   • Spatial semantic clustering\n",
      "   • Semantic controller modules\n",
      "   • ResNet backbone with SOLIDER blocks\n",
      "\n",
      "✅ SOLIDER model successfully exported to ONNX!\n",
      "   You can now visualize it in Netron!\n"
     ]
    }
   ],
   "source": [
    "# # =========================\n",
    "# # ONNX Export for SOLIDER Model (Netron Visualization) - SIMPLIFIED CORE\n",
    "# # =========================\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import os\n",
    "\n",
    "# def export_solider_model_to_onnx():\n",
    "#     \"\"\"\n",
    "#     Export the SOLIDER CNN model to ONNX format for Netron visualization\n",
    "#     \"\"\"\n",
    "#     print(\"Exporting SOLIDER CNN model to ONNX...\")\n",
    "    \n",
    "#     # Create SOLIDER model instance\n",
    "#     solider_model = SOLIDERPersonReIDModel(num_classes=num_classes)\n",
    "    \n",
    "#     # Determine device for dummy input\n",
    "#     if isinstance(device, (list, tuple)):\n",
    "#         dummy_device = f\"cuda:{device[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     else:\n",
    "#         dummy_device = device if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "#     # Move model to device\n",
    "#     solider_model = solider_model.to(dummy_device)\n",
    "#     solider_model.eval()\n",
    "    \n",
    "#     # Create sample input tensor\n",
    "#     batch_size = 1\n",
    "#     sample_input = torch.randn(batch_size, 3, image_height, image_width, device=dummy_device)\n",
    "    \n",
    "#     # Define ONNX export paths\n",
    "#     onnx_dir = \"onnx_models\"\n",
    "#     os.makedirs(onnx_dir, exist_ok=True)\n",
    "    \n",
    "#     # Create a wrapper that exports just the core backbone without the final layers\n",
    "#     class SOLIDERCoreWrapper(nn.Module):\n",
    "#         def __init__(self, model):\n",
    "#             super().__init__()\n",
    "#             self.model = model\n",
    "            \n",
    "#             # Extract the core backbone (stages 0-4)\n",
    "#             self.stage0 = model.stage0\n",
    "#             self.stage1 = model.stage1\n",
    "#             self.stage2 = model.stage2\n",
    "#             self.stage3 = model.stage3\n",
    "#             self.stage4 = model.stage4\n",
    "            \n",
    "#             # Extract multi-scale fusion\n",
    "#             self.multi_scale_fusion = model.multi_scale_fusion\n",
    "            \n",
    "#             # Extract semantic clustering (without final layers)\n",
    "#             self.semantic_clustering = model.semantic_clustering\n",
    "        \n",
    "#         def forward(self, x):\n",
    "#             # Forward through stages\n",
    "#             stage0_out = self.stage0(x)\n",
    "#             stage1_out = self.stage1(stage0_out)\n",
    "#             stage2_out = self.stage2(stage1_out)\n",
    "#             stage3_out = self.stage3(stage2_out)\n",
    "#             stage4_out = self.stage4(stage3_out)\n",
    "            \n",
    "#             # Multi-scale fusion\n",
    "#             fused_features = self.multi_scale_fusion([stage1_out, stage2_out, stage3_out, stage4_out])\n",
    "            \n",
    "#             # Global pooling\n",
    "#             pooled_features = torch.nn.functional.adaptive_avg_pool2d(fused_features, (1, 1))\n",
    "#             pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "            \n",
    "#             # Return intermediate features for visualization\n",
    "#             return pooled_features, stage4_out\n",
    "    \n",
    "#     # Export core architecture\n",
    "#     core_onnx_path = os.path.join(onnx_dir, \"solider_core_architecture.onnx\")\n",
    "#     core_wrapper = SOLIDERCoreWrapper(solider_model)\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Exporting core SOLIDER architecture...\")\n",
    "#         torch.onnx.export(\n",
    "#             core_wrapper,\n",
    "#             sample_input,\n",
    "#             core_onnx_path,\n",
    "#             export_params=True,\n",
    "#             opset_version=11,\n",
    "#             do_constant_folding=True,\n",
    "#             input_names=['input_image'],\n",
    "#             output_names=['pooled_features', 'stage4_features'],\n",
    "#             dynamic_axes={\n",
    "#                 'input_image': {0: 'batch_size'}, \n",
    "#                 'pooled_features': {0: 'batch_size'}, \n",
    "#                 'stage4_features': {0: 'batch_size'}\n",
    "#             },\n",
    "#             verbose=False\n",
    "#         )\n",
    "#         print(f\"✓ SOLIDER core architecture exported to: {core_onnx_path}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Failed to export core architecture: {str(e)}\")\n",
    "        \n",
    "#         # Try an even simpler approach - just the backbone stages\n",
    "#         print(\"Trying simplified backbone export...\")\n",
    "        \n",
    "#         class SOLIDERBackboneWrapper(nn.Module):\n",
    "#             def __init__(self, model):\n",
    "#                 super().__init__()\n",
    "#                 self.stage0 = model.stage0\n",
    "#                 self.stage1 = model.stage1\n",
    "#                 self.stage2 = model.stage2\n",
    "#                 self.stage3 = model.stage3\n",
    "#                 self.stage4 = model.stage4\n",
    "            \n",
    "#             def forward(self, x):\n",
    "#                 x = self.stage0(x)\n",
    "#                 x = self.stage1(x)\n",
    "#                 x = self.stage2(x)\n",
    "#                 x = self.stage3(x)\n",
    "#                 x = self.stage4(x)\n",
    "#                 return x\n",
    "        \n",
    "#         backbone_onnx_path = os.path.join(onnx_dir, \"solider_backbone_stages.onnx\")\n",
    "#         backbone_wrapper = SOLIDERBackboneWrapper(solider_model)\n",
    "        \n",
    "#         try:\n",
    "#             torch.onnx.export(\n",
    "#                 backbone_wrapper,\n",
    "#                 sample_input,\n",
    "#                 backbone_onnx_path,\n",
    "#                 export_params=True,\n",
    "#                 opset_version=11,\n",
    "#                 do_constant_folding=True,\n",
    "#                 input_names=['input_image'],\n",
    "#                 output_names=['backbone_output'],\n",
    "#                 dynamic_axes={\n",
    "#                     'input_image': {0: 'batch_size'}, \n",
    "#                     'backbone_output': {0: 'batch_size'}\n",
    "#                 },\n",
    "#                 verbose=False\n",
    "#             )\n",
    "#             print(f\"✓ SOLIDER backbone stages exported to: {backbone_onnx_path}\")\n",
    "#             core_onnx_path = backbone_onnx_path\n",
    "            \n",
    "#         except Exception as e2:\n",
    "#             print(f\"❌ Failed to export backbone stages: {str(e2)}\")\n",
    "#             return None\n",
    "    \n",
    "#     # Print model statistics\n",
    "#     total_params = sum(p.numel() for p in solider_model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in solider_model.parameters() if p.requires_grad)\n",
    "    \n",
    "#     print(f\"\\n📊 SOLIDER Model Statistics:\")\n",
    "#     print(f\"   • Total parameters: {total_params:,}\")\n",
    "#     print(f\"   • Trainable parameters: {trainable_params:,}\")\n",
    "#     print(f\"   • Input shape: {tuple(sample_input.shape)}\")\n",
    "#     print(f\"   • Number of classes: {num_classes}\")\n",
    "    \n",
    "#     print(f\"\\n🔍 Netron Visualization:\")\n",
    "#     print(f\"   1. Open Netron (https://netron.app/)\")\n",
    "#     print(f\"   2. Load the exported ONNX file: {core_onnx_path}\")\n",
    "    \n",
    "#     print(f\"\\n💡 Key SOLIDER Components to Look For:\")\n",
    "#     print(f\"   • Multi-scale feature fusion (stages 1-4)\")\n",
    "#     print(f\"   • Spatial semantic clustering\")\n",
    "#     print(f\"   • Semantic controller modules\")\n",
    "#     print(f\"   • ResNet backbone with SOLIDER blocks\")\n",
    "    \n",
    "#     return {'core': core_onnx_path}\n",
    "\n",
    "# # Execute the export\n",
    "# try:\n",
    "#     exported_models = export_solider_model_to_onnx()\n",
    "#     if exported_models:\n",
    "#         print(\"\\n✅ SOLIDER model successfully exported to ONNX!\")\n",
    "#         print(\"   You can now visualize it in Netron!\")\n",
    "#     else:\n",
    "#         print(\"\\n❌ Failed to export SOLIDER model\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error during export: {str(e)}\")\n",
    "#     print(\"Make sure the SOLIDERPersonReIDModel class has been defined and all dependencies are imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6d5e6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2025-07-26 18:49:39.641477\n",
      "Using device: cpu\n",
      "FIDI parameters: alpha=1.05, beta=0.5\n",
      "PK sampling: P=16, K=4, batch_size=64\n",
      "Number of classes: 751\n",
      "================================================================================\n",
      "Using device: cpu\n",
      "FIDI parameters: alpha=1.05, beta=0.5\n",
      "SOLIDER model and trainer initialized successfully!\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/250\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'PersonReIDTrainDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m log_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m log_print(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m avg_loss, avg_fidi_loss, avg_ce_loss, avg_semantic_loss, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs_to_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m     53\u001b[0m fidi_losses\u001b[38;5;241m.\u001b[39mappend(avg_fidi_loss)\n",
      "Cell \u001b[0;32mIn[30], line 535\u001b[0m, in \u001b[0;36mSOLIDERFIDITrainer.train_epoch\u001b[0;34m(self, dataloader, epoch, total_epochs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Determine training stage\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_switch_epoch:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch_stage1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_switch_epoch:\n",
      "Cell \u001b[0;32mIn[30], line 562\u001b[0m, in \u001b[0;36mSOLIDERFIDITrainer._train_epoch_stage1\u001b[0;34m(self, dataloader, epoch, total_epochs)\u001b[0m\n\u001b[1;32m    558\u001b[0m batch_semantic_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    560\u001b[0m fidi_weight, cls_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss_weights(epoch, total_epochs)\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    563\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/annsijaz/miniconda3/envs/pytorch/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Training Loop with Live Loss & Accuracy Plot\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up logging to file - ONLY ONCE at the beginning\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "log_file = open(log_filename, 'w')\n",
    "\n",
    "# Create a custom print function that writes to both console and file\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    print(*args, **kwargs, file=log_file)\n",
    "    log_file.flush()  # Ensure immediate writing\n",
    "\n",
    "# Log training start\n",
    "log_print(f\"Training started at: {datetime.now()}\")\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(f\"PK sampling: P={P}, K={K}, batch_size={P*K}\")\n",
    "log_print(f\"Number of classes: {num_classes}\")\n",
    "log_print(\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "fidi_losses = []\n",
    "ce_losses = []\n",
    "semantic_losses = []  # ADD THIS FOR SOLIDER\n",
    "epochs = []\n",
    "rank1s = []\n",
    "maps = []\n",
    "eval_epochs = []\n",
    "\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(\"SOLIDER model and trainer initialized successfully!\")\n",
    "log_print(\"Starting training...\")\n",
    "\n",
    "num_epochs_to_run = num_epochs  # You can override this for shorter runs\n",
    "eval_freq = 10  # You can set this in your config cell if you want\n",
    "\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for epoch in range(num_epochs_to_run):\n",
    "    log_print(f'\\nEpoch {epoch+1}/{num_epochs_to_run}')\n",
    "    log_print('-' * 50)\n",
    "    avg_loss, avg_fidi_loss, avg_ce_loss, avg_semantic_loss, batch_losses, batch_fidi_losses, batch_ce_losses, batch_semantic_losses = trainer.train_epoch(train_loader, epoch, num_epochs_to_run)\n",
    "    train_losses.append(avg_loss)\n",
    "    fidi_losses.append(avg_fidi_loss)\n",
    "    ce_losses.append(avg_ce_loss)\n",
    "    semantic_losses.append(avg_semantic_loss)  # ADD THIS\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # Evaluate and collect accuracy/mAP\n",
    "    if (epoch + 1) % eval_freq == 0 or (epoch + 1) == num_epochs_to_run:\n",
    "        log_print(\"Evaluating...\")\n",
    "        cmc, mAP = trainer.evaluate(query_loader, gallery_loader)\n",
    "        rank1 = float(cmc[0].item())\n",
    "        rank1s.append(rank1)\n",
    "        maps.append(float(mAP))\n",
    "        eval_epochs.append(epoch + 1)\n",
    "        log_print(f'Rank-1: {rank1:.4f}, mAP: {mAP:.4f}')\n",
    "\n",
    "    # Live plot\n",
    "    clear_output(wait=True)\n",
    "    ax1.clear()\n",
    "    ax1.plot(epochs, train_losses, label='Total Loss')\n",
    "    ax1.plot(epochs, fidi_losses, label='FIDI Loss')\n",
    "    ax1.plot(epochs, ce_losses, label='CE Loss')\n",
    "    ax1.plot(epochs, semantic_losses, label='Semantic Loss')  # ADD THIS\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.plot(eval_epochs, rank1s, label='Rank-1 Accuracy')\n",
    "    ax2.plot(eval_epochs, maps, label='mAP')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Validation: Rank-1 & mAP')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    display(fig)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    trainer.scheduler.step()\n",
    "\n",
    "    # Save TorchScript model every 5 epochs\n",
    "    # Save both TorchScript (.pt) and PyTorch (.pth) models every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Create weights directory if it doesn't exist\n",
    "        os.makedirs(\"weights\", exist_ok=True)\n",
    "        \n",
    "        # Save PyTorch state dict (.pth file)\n",
    "        pth_path = f\"weights/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': trainer.model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict()\n",
    "        }, pth_path)\n",
    "        \n",
    "        # Save TorchScript model (.pt file) - also in weights folder\n",
    "        model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "        scripted = torch.jit.script(model_to_save)\n",
    "        script_path = f\"weights/checkpoint_epoch_{epoch+1}.pt\"\n",
    "        scripted.save(script_path)\n",
    "        \n",
    "        log_print(f\"Models saved - PyTorch: {pth_path}, TorchScript: {script_path}\")\n",
    "\n",
    "# Close the log file at the end\n",
    "log_file.close()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "log_print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
