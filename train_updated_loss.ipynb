{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3945b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import itertools\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PKSampler(Sampler):\n",
    "    \"\"\"\n",
    "    PK Sampler for Person Re-ID: P persons × K images per person\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, P=16, K=4):\n",
    "        self.data_source = data_source\n",
    "        self.P = P  # Number of persons per batch\n",
    "        self.K = K  # Number of images per person\n",
    "        \n",
    "        # Group samples by person ID\n",
    "        self.pid_to_indices = defaultdict(list)\n",
    "        for idx, (_, pid) in enumerate(data_source.samples):\n",
    "            self.pid_to_indices[pid].append(idx)\n",
    "        \n",
    "        # Filter out persons with less than K images\n",
    "        self.valid_pids = [pid for pid, indices in self.pid_to_indices.items() \n",
    "                          if len(indices) >= self.K]\n",
    "        \n",
    "        if len(self.valid_pids) < self.P:\n",
    "            raise ValueError(f\"Not enough persons with at least {self.K} images. \"\n",
    "                           f\"Found {len(self.valid_pids)}, need {self.P}\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Calculate number of batches\n",
    "        num_batches = len(self.valid_pids) // self.P\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            # Randomly select P persons\n",
    "            selected_pids = random.sample(self.valid_pids, self.P)\n",
    "            \n",
    "            batch_indices = []\n",
    "            for pid in selected_pids:\n",
    "                # Randomly select K images for this person\n",
    "                available_indices = self.pid_to_indices[pid]\n",
    "                selected_indices = random.sample(available_indices, \n",
    "                                               min(self.K, len(available_indices)))\n",
    "                batch_indices.extend(selected_indices)\n",
    "            \n",
    "            # Shuffle within batch to avoid ordering bias\n",
    "            random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_pids) // self.P\n",
    "\n",
    "class PersonReIDTrainDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training set: expects structure Dataset/train/<pid>/*.jpg\n",
    "    Returns (image, label)\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label)\n",
    "        self.label_map = {}  # pid (str) -> label (int)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        pids = sorted(os.listdir(self.root_dir))\n",
    "        self.label_map = {pid: idx for idx, pid in enumerate(pids)}\n",
    "        for pid in pids:\n",
    "            pid_dir = os.path.join(self.root_dir, pid)\n",
    "            if not os.path.isdir(pid_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(pid_dir):\n",
    "                if fname.lower().endswith('.jpg'):\n",
    "                    self.samples.append((os.path.join(pid_dir, fname), self.label_map[pid]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "class PersonReIDTestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for query/gallery set: expects structure Dataset/query/*.jpg or Dataset/gallery/*.jpg\n",
    "    Returns (image, label, cam_id)\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.samples = []  # List of (img_path, label, cam_id)\n",
    "        self._prepare()\n",
    "\n",
    "    def _prepare(self):\n",
    "        for fname in os.listdir(self.dir_path):\n",
    "            if fname.lower().endswith('.jpg'):\n",
    "                # Example: 0001_c1s1_001051_00.jpg\n",
    "                parts = fname.split('_')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                label = int(parts[0])\n",
    "                cam_id = int(parts[1][1])  # e.g., c1 -> 1\n",
    "                self.samples.append((os.path.join(self.dir_path, fname), label, cam_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, cam_id = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label, cam_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "956330f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Model Definition\n",
    "# =========================\n",
    "class PersonReIDModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Person Re-identification model with ResNet50 backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feature_dim=2048):\n",
    "        super(PersonReIDModel, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.bn_neck = nn.BatchNorm1d(feature_dim)\n",
    "        self.bn_neck.bias.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes, bias=False)\n",
    "        self._init_params()\n",
    "    def _init_params(self):\n",
    "        nn.init.kaiming_normal_(self.classifier.weight, mode='fan_out')\n",
    "        nn.init.constant_(self.bn_neck.weight, 1)\n",
    "        nn.init.constant_(self.bn_neck.bias, 0)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        features = self.bn_neck(x)\n",
    "        logits = self.classifier(features)\n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDILoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Fine-grained Difference-aware (FIDI) Pairwise Loss\n",
    "    Corrected implementation following the paper exactly\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.05, beta=0.5):\n",
    "        super(FIDILoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: tensor of shape (batch_size, feature_dim)\n",
    "            labels: tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = self.compute_pairwise_distances(features)\n",
    "        \n",
    "        # Compute ground truth binary relationship matrix K\n",
    "        labels = labels.view(-1, 1)\n",
    "        k_matrix = (labels == labels.T).float()  # 1 if same identity, 0 otherwise\n",
    "        \n",
    "        # Compute learned probability distribution U using exponential function\n",
    "        u_matrix = torch.exp(-self.beta * distances)\n",
    "        \n",
    "        # Compute D(U||K) + D(K||U)\n",
    "        d_u_k = self.compute_kl_divergence(u_matrix, k_matrix)\n",
    "        d_k_u = self.compute_kl_divergence(k_matrix, u_matrix)\n",
    "        \n",
    "        total_loss = d_u_k + d_k_u\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_pairwise_distances(self, features):\n",
    "        \"\"\"Compute Euclidean distances between all pairs of features\"\"\"\n",
    "        n = features.size(0)\n",
    "        # Expand features to compute all pairwise distances\n",
    "        features_1 = features.unsqueeze(1).expand(n, n, -1)\n",
    "        features_2 = features.unsqueeze(0).expand(n, n, -1)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distances = torch.sqrt(torch.sum((features_1 - features_2) ** 2, dim=2) + self.eps)\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def compute_kl_divergence(self, p_matrix, q_matrix):\n",
    "        \"\"\"\n",
    "        Compute KL divergence D(P||Q) following Equation (5) from the paper:\n",
    "        D(P||Q) = Σ p_ij * log(α * p_ij / ((α-1) * p_ij + q_ij))\n",
    "        \"\"\"\n",
    "        # Clamp to avoid numerical issues\n",
    "        p_matrix = torch.clamp(p_matrix, min=self.eps, max=1-self.eps)\n",
    "        q_matrix = torch.clamp(q_matrix, min=self.eps, max=1-self.eps)\n",
    "        \n",
    "        # Compute the denominator: (α-1) * p_ij + q_ij\n",
    "        denominator = (self.alpha - 1) * p_matrix + q_matrix\n",
    "        denominator = torch.clamp(denominator, min=self.eps)\n",
    "        \n",
    "        # Compute the fraction: α * p_ij / denominator\n",
    "        numerator = self.alpha * p_matrix\n",
    "        fraction = numerator / denominator\n",
    "        fraction = torch.clamp(fraction, min=self.eps)\n",
    "        \n",
    "        # Compute KL divergence: p_ij * log(fraction)\n",
    "        kl_div = p_matrix * torch.log(fraction)\n",
    "        \n",
    "        # Exclude diagonal elements (self-comparisons) and compute mean\n",
    "        mask = ~torch.eye(p_matrix.size(0), dtype=torch.bool, device=p_matrix.device)\n",
    "        kl_div = kl_div[mask].mean()\n",
    "        \n",
    "        return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e36ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Trainer Class\n",
    "# =========================\n",
    "class FIDITrainer:\n",
    "    \"\"\"\n",
    "    Improved Training framework for Person Re-ID with FIDI loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_classes, device='cuda', \n",
    "                 alpha=1.05, beta=0.5, lr=3.5e-4, weight_decay=5e-4,\n",
    "                 loss_strategy='adaptive'):\n",
    "        # Multi-GPU support\n",
    "        if isinstance(device, (list, tuple)):\n",
    "            assert torch.cuda.is_available(), \"CUDA must be available for multi-GPU.\"\n",
    "            self.device = torch.device(f\"cuda:{device[0]}\")\n",
    "            model = model.to(self.device)\n",
    "            self.model = nn.DataParallel(model, device_ids=device)\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            self.model = model.to(self.device)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fidi_loss = FIDILoss(alpha=alpha, beta=beta)\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.loss_strategy = loss_strategy\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=40, gamma=0.1\n",
    "        )\n",
    "        \n",
    "        # For adaptive strategy\n",
    "        self.loss_history = {'fidi': [], 'ce': []}\n",
    "        self.best_mAP = 0.0\n",
    "    \n",
    "    def get_loss_weights(self, epoch, total_epochs, strategy=None):\n",
    "        \"\"\"\n",
    "        Multiple loss weighting strategies based on training progress and loss magnitudes\n",
    "        \"\"\"\n",
    "        if strategy is None:\n",
    "            strategy = self.loss_strategy\n",
    "            \n",
    "        progress = epoch / total_epochs\n",
    "        \n",
    "        if strategy == 'conservative':\n",
    "            # More conservative approach - slower FIDI ramp-up, maintain CE importance\n",
    "            fidi_weight = min(0.8, progress * 1.5)  # Max 0.8, reaches it at 53% of training\n",
    "            cls_weight = max(0.8, 1.2 - progress)   # Min 0.8, gradual decrease\n",
    "            \n",
    "        elif strategy == 'progressive':\n",
    "            # Gradual transition with smooth curves\n",
    "            import math\n",
    "            fidi_weight = 0.5 * (1 + math.tanh(4 * (progress - 0.5)))  # Sigmoid-like curve\n",
    "            cls_weight = 1.0 - 0.3 * progress  # Linear decrease to 0.7\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adaptive based on loss magnitudes (requires loss history)\n",
    "            if len(self.loss_history['fidi']) > 5:\n",
    "                # Calculate recent loss ratios\n",
    "                recent_fidi = sum(self.loss_history['fidi'][-5:]) / 5\n",
    "                recent_ce = sum(self.loss_history['ce'][-5:]) / 5\n",
    "                \n",
    "                # Balance weights based on loss magnitudes\n",
    "                if recent_fidi > recent_ce * 2:  # FIDI much larger\n",
    "                    fidi_weight = max(0.3, min(0.7, 0.5 - 0.2 * (recent_fidi / recent_ce - 2)))\n",
    "                    cls_weight = 1.0\n",
    "                elif recent_ce > recent_fidi * 2:  # CE much larger\n",
    "                    fidi_weight = min(1.0, 0.5 + 0.3 * (recent_ce / recent_fidi - 2))\n",
    "                    cls_weight = max(0.7, 1.0 - 0.2 * (recent_ce / recent_fidi - 2))\n",
    "                else:  # Balanced\n",
    "                    fidi_weight = 0.5 + 0.3 * progress\n",
    "                    cls_weight = 1.0 - 0.2 * progress\n",
    "            else:\n",
    "                # Early training fallback\n",
    "                fidi_weight = 0.3 + 0.3 * progress\n",
    "                cls_weight = 1.0\n",
    "                \n",
    "        elif strategy == 'fixed':\n",
    "            # Simple fixed weights\n",
    "            fidi_weight = 0.7\n",
    "            cls_weight = 1.0\n",
    "            \n",
    "        else:  # 'original' - your current strategy\n",
    "            fidi_weight = min(1.0, epoch / (total_epochs * 0.3))\n",
    "            cls_weight = max(0.5, 1.0 - epoch / (total_epochs * 0.8))\n",
    "        \n",
    "        return fidi_weight, cls_weight\n",
    "    \n",
    "    def train_epoch(self, dataloader, epoch=0, total_epochs=120):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_fidi_loss = 0.0\n",
    "        total_ce_loss = 0.0\n",
    "        batch_losses = []\n",
    "        batch_fidi_losses = []\n",
    "        batch_ce_losses = []\n",
    "        \n",
    "        # Get dynamic weights for this epoch\n",
    "        fidi_weight, cls_weight = self.get_loss_weights(epoch, total_epochs)\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            features, logits = self.model(images)\n",
    "            fidi_loss = self.fidi_loss(features, labels)\n",
    "            ce_loss = self.ce_loss(logits, labels)\n",
    "            \n",
    "            # Apply dynamic weighting\n",
    "            loss = fidi_weight * fidi_loss + cls_weight * ce_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store all batch values\n",
    "            batch_loss = loss.item()\n",
    "            batch_fidi = fidi_loss.item()\n",
    "            batch_ce = ce_loss.item()\n",
    "            \n",
    "            total_loss += batch_loss\n",
    "            total_fidi_loss += batch_fidi\n",
    "            total_ce_loss += batch_ce\n",
    "            \n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_fidi_losses.append(batch_fidi)\n",
    "            batch_ce_losses.append(batch_ce)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}: Loss={batch_loss:.6f}, '\n",
    "                      f'FIDI={batch_fidi:.6f}×{fidi_weight:.2f}, '\n",
    "                      f'CE={batch_ce:.6f}×{cls_weight:.2f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_fidi_loss = total_fidi_loss / len(dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "        \n",
    "        # Calculate additional statistics\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        std_loss = np.std(batch_losses)\n",
    "        \n",
    "        print(f'Epoch Summary: Avg Loss={avg_loss:.6f}, Min={min_loss:.6f}, Max={max_loss:.6f}, Std={std_loss:.6f}')\n",
    "        print(f'FIDI: Avg={avg_fidi_loss:.6f}, Min={min(batch_fidi_losses):.6f}, Max={max(batch_fidi_losses):.6f}')\n",
    "        print(f'CE: Avg={avg_ce_loss:.6f}, Min={min(batch_ce_losses):.6f}, Max={max(batch_ce_losses):.6f}')\n",
    "        \n",
    "        # Store loss history for adaptive strategy\n",
    "        self.loss_history['fidi'].append(avg_fidi_loss)\n",
    "        self.loss_history['ce'].append(avg_ce_loss)\n",
    "        if len(self.loss_history['fidi']) > 20:  # Keep only recent history\n",
    "            self.loss_history['fidi'].pop(0)\n",
    "            self.loss_history['ce'].pop(0)\n",
    "        \n",
    "        return avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses\n",
    "    \n",
    "    def evaluate(self, query_dataloader, gallery_dataloader):\n",
    "        self.model.eval()\n",
    "        query_features = []\n",
    "        query_labels = []\n",
    "        query_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in query_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                query_features.append(features.cpu())\n",
    "                query_labels.extend(labels.numpy())\n",
    "                query_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        query_features = torch.cat(query_features, dim=0)\n",
    "        query_features = F.normalize(query_features, p=2, dim=1)\n",
    "        \n",
    "        gallery_features = []\n",
    "        gallery_labels = []\n",
    "        gallery_cam_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, cam_ids in gallery_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                features, _ = self.model(images)\n",
    "                gallery_features.append(features.cpu())\n",
    "                gallery_labels.extend(labels.numpy())\n",
    "                gallery_cam_ids.extend(cam_ids.numpy())\n",
    "        \n",
    "        gallery_features = torch.cat(gallery_features, dim=0)\n",
    "        gallery_features = F.normalize(gallery_features, p=2, dim=1)\n",
    "        \n",
    "        dist_matrix = torch.cdist(query_features, gallery_features, p=2)\n",
    "        cmc, mAP = self.compute_cmc_map(\n",
    "            dist_matrix, query_labels, gallery_labels, \n",
    "            query_cam_ids, gallery_cam_ids\n",
    "        )\n",
    "        \n",
    "        return cmc, mAP\n",
    "    \n",
    "    def compute_cmc_map(self, dist_matrix, query_labels, gallery_labels, \n",
    "                       query_cam_ids, gallery_cam_ids, max_rank=50):\n",
    "        num_q, num_g = dist_matrix.shape\n",
    "        if num_g < max_rank:\n",
    "            max_rank = num_g\n",
    "            print(f\"Note: number of gallery samples is quite small, got {num_g}\")\n",
    "        \n",
    "        indices = torch.argsort(dist_matrix, dim=1)\n",
    "        matches = (torch.tensor(gallery_labels)[indices] == \n",
    "                  torch.tensor(query_labels).view(-1, 1))\n",
    "        \n",
    "        all_cmc = []\n",
    "        all_AP = []\n",
    "        num_valid_q = 0\n",
    "        \n",
    "        for q_idx in range(num_q):\n",
    "            q_pid = query_labels[q_idx]\n",
    "            q_camid = query_cam_ids[q_idx]\n",
    "            order = indices[q_idx]\n",
    "            \n",
    "            remove = torch.tensor([(gallery_labels[i] == q_pid) & \n",
    "                                 (gallery_cam_ids[i] == q_camid) \n",
    "                                 for i in order])\n",
    "            keep = ~remove\n",
    "            orig_cmc = matches[q_idx][keep]\n",
    "            \n",
    "            if not torch.any(orig_cmc):\n",
    "                continue\n",
    "            \n",
    "            cmc = orig_cmc.cumsum(0)\n",
    "            cmc[cmc > 1] = 1\n",
    "            all_cmc.append(cmc[:max_rank])\n",
    "            num_valid_q += 1\n",
    "            \n",
    "            num_rel = orig_cmc.sum()\n",
    "            tmp_cmc = orig_cmc.cumsum(0)\n",
    "            tmp_cmc = tmp_cmc / (torch.arange(len(tmp_cmc)) + 1.0)\n",
    "            tmp_cmc = tmp_cmc * orig_cmc\n",
    "            AP = tmp_cmc.sum() / num_rel\n",
    "            all_AP.append(AP)\n",
    "        \n",
    "        if num_valid_q == 0:\n",
    "            raise RuntimeError(\"No valid query\")\n",
    "        \n",
    "        all_cmc = torch.stack(all_cmc, dim=0).float()\n",
    "        all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "        mAP = sum(all_AP) / len(all_AP)\n",
    "        \n",
    "        return all_cmc, mAP\n",
    "    \n",
    "    def train(self, train_dataloader, query_dataloader, gallery_dataloader, \n",
    "              num_epochs=120, eval_freq=10):\n",
    "        print(f\"Starting training with '{self.loss_strategy}' loss weighting strategy...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Get current weights for logging\n",
    "            fidi_weight, cls_weight = self.get_loss_weights(epoch, num_epochs)\n",
    "            print(f'Loss weights - FIDI: {fidi_weight:.3f}, CE: {cls_weight:.3f}')\n",
    "            \n",
    "            avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses = self.train_epoch(\n",
    "                train_dataloader, epoch, num_epochs\n",
    "            )\n",
    "            print(f'Train Loss: {avg_loss:.4f}, FIDI Loss: {avg_fidi_loss:.4f}, '\n",
    "                  f'CE Loss: {avg_ce_loss:.4f}')\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % eval_freq == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                cmc, mAP = self.evaluate(query_dataloader, gallery_dataloader)\n",
    "                print(f'Rank-1: {cmc[0]:.4f}, Rank-5: {cmc[4]:.4f}, '\n",
    "                      f'Rank-10: {cmc[9]:.4f}, mAP: {mAP:.4f}')\n",
    "                \n",
    "                if mAP > self.best_mAP:\n",
    "                    self.best_mAP = mAP\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                        'mAP': mAP,\n",
    "                        'cmc': cmc,\n",
    "                        'loss_strategy': self.loss_strategy,\n",
    "                        'fidi_weight': fidi_weight,\n",
    "                        'cls_weight': cls_weight,\n",
    "                    }, 'best_model.pth')\n",
    "                    print(f'New best mAP: {self.best_mAP:.4f}')\n",
    "        \n",
    "        print(f'\\nTraining completed. Best mAP: {self.best_mAP:.4f}')\n",
    "        return self.best_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e9c6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Tune-able Parameters / Config\n",
    "# =========================\n",
    "# PK Sampling parameters\n",
    "P = 16  # Number of persons per batch\n",
    "K = 4   # Number of images per person\n",
    "batch_size = P * K  # This will be 64 for optimal PK sampling\n",
    "\n",
    "num_epochs = 250\n",
    "device = [0, 1] if torch.cuda.device_count() > 1 else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alpha = 1.05\n",
    "beta = 0.5\n",
    "lr = 3.5e-4\n",
    "weight_decay = 5e-4\n",
    "num_workers = 8\n",
    "prefetch_factor = 4\n",
    "image_height = 256\n",
    "image_width = 128\n",
    "train_dir = os.path.join('Dataset', 'train')\n",
    "query_dir = os.path.join('Dataset', 'query')\n",
    "gallery_dir = os.path.join('Dataset', 'gallery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdd83123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. Data Transforms & DataLoaders   ←  REPLACE THIS CELL\n",
    "# =========================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.Pad(10, padding_mode='edge'),      # add border for random crop\n",
    "    transforms.RandomCrop((image_height, image_width)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2, contrast=0.15, saturation=0.15, hue=0.1\n",
    "    ),                                            # lightweight colour aug\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(                     # ← paper-recommended\n",
    "        p=0.5, scale=(0.02, 0.4), ratio=(0.3, 3.3),\n",
    "        value='random'\n",
    "    ),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((image_height, image_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset   = PersonReIDTrainDataset(train_dir,  transform=train_transform)\n",
    "query_dataset   = PersonReIDTestDataset (query_dir,  transform=test_transform)\n",
    "gallery_dataset = PersonReIDTestDataset (gallery_dir, transform=test_transform)\n",
    "\n",
    "num_classes = len(train_dataset.label_map)\n",
    "\n",
    "pk_sampler = PKSampler(train_dataset, P=P, K=K)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=pk_sampler,          # PK sampling\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "query_loader = DataLoader(\n",
    "    query_dataset,   batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor\n",
    ")\n",
    "gallery_loader = DataLoader(\n",
    "    gallery_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True, prefetch_factor=prefetch_factor\n",
    ")\n",
    "\n",
    "print(f\"Training with PK sampling: P={P}, K={K}, batch={P*K}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3092619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anns/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anns/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 8. Model & Trainer Initialization\n",
    "# =========================\n",
    "model = PersonReIDModel(num_classes=num_classes)\n",
    "\n",
    "# =========================\n",
    "# 8b. Export Model to ONNX for Netron (before training)\n",
    "# =========================\n",
    "import torch\n",
    "onnx_path = \"person_reid_model.onnx\"\n",
    "# Determine device for dummy input\n",
    "if isinstance(device, (list, tuple)):\n",
    "    dummy_device = f\"cuda:{device[0]}\" if torch.cuda.is_available() else \"cpu\"\n",
    "else:\n",
    "    dummy_device = device\n",
    "# Create a sample input\n",
    "model = model.to(dummy_device)\n",
    "sample_input = torch.randn(1, 3, image_height, image_width, device=dummy_device)\n",
    "# If using DataParallel, get the underlying model\n",
    "export_model = model.module if hasattr(model, \"module\") else model\n",
    "# Export the model\n",
    "export_model.eval()\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    sample_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['features', 'logits'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'features': {0: 'batch_size'}, 'logits': {0: 'batch_size'}}\n",
    ")\n",
    "print(f\"Model exported to {onnx_path}. You can now open it in Netron for architecture visualization.\")\n",
    "\n",
    "trainer = FIDITrainer(\n",
    "    model=model,\n",
    "    num_classes=num_classes,\n",
    "    device=device,\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    loss_strategy='progressive'\n",
    ")\n",
    "\n",
    "# # For challenging datasets (CUHK03)\n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='conservative')\n",
    "\n",
    "# # For easier datasets (Market1501) \n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='progressive')\n",
    "\n",
    "# # For experimental/research purposes\n",
    "# trainer = FIDITrainer(model, num_classes, loss_strategy='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5e6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m log_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs_to_run\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m log_print(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m avg_loss, avg_fidi_loss, avg_ce_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_epoch(train_loader, epoch, num_epochs_to_run)\n\u001b[1;32m     54\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m     55\u001b[0m fidi_losses\u001b[38;5;241m.\u001b[39mappend(avg_fidi_loss)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Training Loop with Live Loss & Accuracy Plot\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging to file\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "log_file = open(log_filename, 'w')\n",
    "\n",
    "# Create a custom print function that writes to both console and file\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    print(*args, **kwargs, file=log_file)\n",
    "    log_file.flush()  # Ensure immediate writing\n",
    "\n",
    "# Redirect stdout to capture all print statements\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = log_file\n",
    "\n",
    "# Log training start\n",
    "log_print(f\"Training started at: {datetime.now()}\")\n",
    "log_print(f\"Using device: {device}\")\n",
    "log_print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "log_print(f\"PK sampling: P={P}, K={K}, batch_size={P*K}\")\n",
    "log_print(f\"Number of classes: {num_classes}\")\n",
    "log_print(\"=\"*80)\n",
    "\n",
    "train_losses = []\n",
    "fidi_losses = []\n",
    "ce_losses = []\n",
    "epochs = []\n",
    "rank1s = []\n",
    "maps = []\n",
    "eval_epochs = []\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"FIDI parameters: alpha={alpha}, beta={beta}\")\n",
    "print(\"Model and trainer initialized successfully!\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "num_epochs_to_run = num_epochs  # You can override this for shorter runs\n",
    "eval_freq = 10  # You can set this in your config cell if you want\n",
    "\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for epoch in range(num_epochs_to_run):\n",
    "    log_print(f'\\nEpoch {epoch+1}/{num_epochs_to_run}')\n",
    "    log_print('-' * 50)\n",
    "    avg_loss, avg_fidi_loss, avg_ce_loss, batch_losses, batch_fidi_losses, batch_ce_losses = trainer.train_epoch(train_loader, epoch, num_epochs_to_run)\n",
    "    train_losses.append(avg_loss)\n",
    "    fidi_losses.append(avg_fidi_loss)\n",
    "    ce_losses.append(avg_ce_loss)\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # Evaluate and collect accuracy/mAP\n",
    "    if (epoch + 1) % eval_freq == 0 or (epoch + 1) == num_epochs_to_run:\n",
    "        log_print(\"Evaluating...\")\n",
    "        cmc, mAP = trainer.evaluate(query_loader, gallery_loader)\n",
    "        rank1 = float(cmc[0].item())\n",
    "        rank1s.append(rank1)\n",
    "        maps.append(float(mAP))\n",
    "        eval_epochs.append(epoch + 1)\n",
    "        log_print(f'Rank-1: {rank1:.4f}, mAP: {mAP:.4f}')\n",
    "\n",
    "    # Live plot\n",
    "    clear_output(wait=True)\n",
    "    ax1.clear()\n",
    "    ax1.plot(epochs, train_losses, label='Total Loss')\n",
    "    ax1.plot(epochs, fidi_losses, label='FIDI Loss')\n",
    "    ax1.plot(epochs, ce_losses, label='CE Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Losses')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.plot(eval_epochs, rank1s, label='Rank-1 Accuracy')\n",
    "    ax2.plot(eval_epochs, maps, label='mAP')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Validation: Rank-1 & mAP')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    display(fig)\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    trainer.scheduler.step()\n",
    "\n",
    "    # Save TorchScript model every 5 epochs\n",
    "    # Save both TorchScript (.pt) and PyTorch (.pth) models every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Create weights directory if it doesn't exist\n",
    "        os.makedirs(\"weights\", exist_ok=True)\n",
    "        \n",
    "        # Save PyTorch state dict (.pth file)\n",
    "        pth_path = f\"weights/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': trainer.model.state_dict(),\n",
    "            'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': trainer.scheduler.state_dict()\n",
    "        }, pth_path)\n",
    "        \n",
    "        # Save TorchScript model (.pt file) - also in weights folder\n",
    "        model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "        scripted = torch.jit.script(model_to_save)\n",
    "        script_path = f\"weights/checkpoint_epoch_{epoch+1}.pt\"\n",
    "        scripted.save(script_path)\n",
    "        \n",
    "        print(f\"Models saved - PyTorch: {pth_path}, TorchScript: {script_path}\")\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
